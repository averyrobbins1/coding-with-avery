[
  {
    "path": "posts/2021-02-22-ggtext/",
    "title": "Level up your ggplot2 graphics with ggtext",
    "description": "ggplot2 is arguably one of the most powerful plotting tools of any programming language. Built upon a solid grammar of graphics, ggplot2 allows you to quickly create highly customizable data visualizations. The ggtext package extends the bounds for customization even further.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2021-02-22",
    "categories": [],
    "contents": "\r\n\r\nPreface\r\nFor my senior project during my last semester at BYU-Idaho, my friend (also studying data science) and I are teaming up to build a hopefully helpful tool for BYU-Idaho’s Office of Teacher Preparation (OTP). The OTP is the part of BYU-I that is responsible for sending out teachers (for elementary school, high school, etc.) into the world. In order for a student, or teacher candidate, to become a fully certified teacher, they have to pass a number of Praxis exams. These are simply standardized tests that have been developed to gauge a teacher candidate’s capabilities and knowledge in different categories ranging from english to math.\r\nMy friend and I are building a web application that uses machine learning to give students a better picture of how likely they are to pass these exams. Exams cost money to take and effort to prepare for, so we hope to save students both time and money. We hope to be able to point them towards areas where they can better focus their preparation in order to increase their chances of passing.\r\nWhat they say about data science projects is true: the bulk of our work thus far has been wrangling and tidying the data, as well as feature engineering and selection. Among other features that we have access to, we hope to use students’ past scores on standardized tests (for example, the ACT - see plot above) to help predict their scores on future tests. From the relationships shown in the data in the plot above, we are hopeful that ACT scores will be a fairly decent predictor.\r\nAs part of our project, we want to turn some of our exploratory plots into graphics for communication to show to faculty and other persons involved in what we are trying to accomplish. Hence the creation of the plot above.\r\nDiving into the code\r\nThe rest of this post will be focused on using the ggtext extension to ggplot2, as well as customizing a drab ggplot2 graphic by editing theme components.\r\nBecause of the sensitive nature of the data we are working with, I will instead demonstrate some of ggtext’s and ggplot2’s capabilities using the mpg dataset from ggplot2. We will create a similar plot to the one shown above, and this should allow you to follow along and mess with the code as desired.\r\nFirst, we need to install and load the following packages:\r\n\r\n\r\n# install.packages(c('dplyr', 'ggplot2', 'ggtext', 'showtext', 'glue'))\r\nlibrary(dplyr)    # any data manipulation\r\nlibrary(ggplot2)  # plotting\r\nlibrary(ggtext)   # customizing text in ggplot2 plots\r\nlibrary(showtext) # awesome package for fonts\r\nlibrary(glue)     # for gluing together code and text\r\n\r\n\r\n\r\nNext, we’ll load the data from ggplot2.\r\n\r\n\r\ndat <- ggplot2::mpg\r\n\r\n# run ?ggplot2::mpg in the R console\r\n# if you want to learn more about the data\r\n\r\nglimpse(dat)\r\n\r\n\r\nRows: 234\r\nColumns: 11\r\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"aud...\r\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a...\r\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8,...\r\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 19...\r\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6,...\r\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"aut...\r\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\",...\r\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, ...\r\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, ...\r\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\",...\r\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", ...\r\n\r\nFor our plot, we will compare city miles per gallon (cty) against highway miles per gallon (hwy), faceted by number of cylinders (cyl). In this toy example, we will also use filter to exclude five cylinder vehicles, which are pretty rare.\r\n\r\n\r\ndat %>% count(cyl)\r\n\r\n\r\n# A tibble: 4 x 2\r\n    cyl     n\r\n* <int> <int>\r\n1     4    81\r\n2     5     4\r\n3     6    79\r\n4     8    70\r\n\r\ndat <- dat %>% \r\n  filter(cyl != 5)\r\n\r\ndat %>% count(cyl)\r\n\r\n\r\n# A tibble: 3 x 2\r\n    cyl     n\r\n* <int> <int>\r\n1     4    81\r\n2     6    79\r\n3     8    70\r\n\r\nPerhaps the most basic and simple version of the plot we could make with ggplot2 would be the following:\r\n\r\n\r\ndat %>% \r\n  ggplot() +\r\n  geom_point(aes(x = cty, y = hwy)) +\r\n  facet_wrap(~ cyl)\r\n\r\n\r\n\r\n\r\nNow onto the customization!\r\nI recently discovered the showtext package, which makes it ridiculously easy to add different fonts to R graphs. We will use font_add_google to load the specific font that we want and showtext_auto to automatically use showtext for new graphic devices.\r\n\r\n\r\nfont_add_google('Fira Code')\r\nshowtext_auto()\r\n\r\n\r\n\r\nNext, we will do a little more data manipulation to get our colors into dat. Finding a good color picker website is a must, and recently I have enjoyed using this one. After finding the right colors and their corresponding HTML codes, we’ll use mutate and case_when to create two new columns: one for displaying the cyl as words, and one to match the colors to the right values of cyl.\r\n\r\n\r\ndat <- dat %>% \r\n    mutate(\r\n        cyl_text = case_when(\r\n            cyl == 4 ~ 'Four',\r\n            cyl == 6 ~ 'Six',\r\n            cyl == 8 ~ 'Eight'\r\n        ),\r\n        cyl_color = case_when(\r\n            cyl == 4 ~ '#4E598A',\r\n            cyl == 6 ~ '#598A4E',\r\n            cyl == 8 ~ '#8A4E59'\r\n        )\r\n    )\r\n\r\nglimpse(dat)\r\n\r\n\r\nRows: 230\r\nColumns: 13\r\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"aud...\r\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a...\r\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8,...\r\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 19...\r\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6,...\r\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"aut...\r\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\",...\r\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, ...\r\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, ...\r\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\",...\r\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", ...\r\n$ cyl_text     <chr> \"Four\", \"Four\", \"Four\", \"Four\", \"Six\", \"Six\"...\r\n$ cyl_color    <chr> \"#4E598A\", \"#4E598A\", \"#4E598A\", \"#4E598A\", ...\r\n\r\nNow we will use glue and just a little bit of CSS to prepare the column that we will use for faceting. By doing this, the facet labels on the plot will be colored appropriately. If you have never seen glue or inline CSS before, check out glue or inline CSS. You certainly don’t need to know a whole lot of HTML or CSS to use ggtext, but just a little bit of knowledge will go a long way. w3schools is a great resource for that kind of stuff.\r\n\r\n\r\ndat <- dat %>% \r\n    mutate(\r\n        cyl_text = glue(\"<b style='color:{cyl_color}'>{cyl_text}<\/b>\")\r\n    )\r\n\r\n# notice we just overrode the old cyl_text column\r\n# with the same cyl_text column coupled with\r\n# html/css styling. If that doesn't make sense now,\r\n# hopefully it will later.\r\n\r\nglimpse(dat)\r\n\r\n\r\nRows: 230\r\nColumns: 13\r\n$ manufacturer <chr> \"audi\", \"audi\", \"audi\", \"audi\", \"audi\", \"aud...\r\n$ model        <chr> \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a4\", \"a...\r\n$ displ        <dbl> 1.8, 1.8, 2.0, 2.0, 2.8, 2.8, 3.1, 1.8, 1.8,...\r\n$ year         <int> 1999, 1999, 2008, 2008, 1999, 1999, 2008, 19...\r\n$ cyl          <int> 4, 4, 4, 4, 6, 6, 6, 4, 4, 4, 4, 6, 6, 6, 6,...\r\n$ trans        <chr> \"auto(l5)\", \"manual(m5)\", \"manual(m6)\", \"aut...\r\n$ drv          <chr> \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"f\", \"4\", \"4\",...\r\n$ cty          <int> 18, 21, 20, 21, 16, 18, 18, 18, 16, 20, 19, ...\r\n$ hwy          <int> 29, 29, 31, 30, 26, 26, 27, 26, 25, 28, 27, ...\r\n$ fl           <chr> \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\",...\r\n$ class        <chr> \"compact\", \"compact\", \"compact\", \"compact\", ...\r\n$ cyl_text     <glue> \"<b style='color:#4E598A'>Four<\/b>\", \"<b st...\r\n$ cyl_color    <chr> \"#4E598A\", \"#4E598A\", \"#4E598A\", \"#4E598A\", ...\r\n\r\nA lot of what we will do involves modifying individual theme components. This can be somewhat tedious. I often use the docs as a reference, but some people also like the ggeasy package.\r\nBefore we get to the main code for the plot, let me point out a couple of theme elements, element_textbox_simple and element_markdown , that are unique to ggtext.\r\nggtext can handle attributes of markdown, HTML, and CSS that you may want to throw into the title, subtitle, labels, etc., but it will not recognize it automatically. Therefore, instead of using element_text in the theme function, you want to use element_markdown. This will make it so everything renders how you would expect it to. As mentioned, I also used element_textbox_simple to convert the title into a simple text box. What an aptly named function. Well done, Claus.\r\nTo keep things less cluttered in the code below, let’s write out the title, subtitle, and caption here, and store them in variables. You should see that we are again using some simple HTML and CSS for coloring and styling certain bits of text.\r\n\r\n\r\ntitle <- \"Vehicles with <b style = 'color:#4E598A;'>four<\/b> cylinder engines get better gas mileage, on average,<br>than those with <b style = 'color:#598A4E;'>six<\/b> or <b style = 'color:#8A4E59;'>eight<\/b>\"\r\n\r\nsubtitle <- \"Most cars are powered by a <b style = 'color:#4E598A;'>four<\/b> or <b style = 'color:#598A4E;'>six<\/b> cylinder engine, while most trucks have a <b style = 'color:#598A4E;'>six<\/b> or <b style = 'color:#8A4E59;'>eight<\/b> cylinder. The more cylinders in an engine, the more combustion that occurs, creating more movement to turn the crankshaft and power to move the car. <span style = 'color:#473504;'>However, more cylinders also require more gasoline to make the combustion necessary to drive the car and thus are not as efficient.<\/span> - Eagle Ridge\"\r\n\r\ncaption <- 'Data: mpg dataset in ggplot2 R package\\n\r\nSource: www\\\\.eagleridgegm\\\\.com/the\\\\-difference\\\\-between\\\\-4\\\\-cylinder\\\\-v6\\\\-v8\\\\-engines'\r\n\r\n\r\n\r\nNow instead of breaking the following chunk up into a bunch of pieces, I will do two things to hopefully help the reader understand what is going on: 1) comments interspersed throughout the code should help a lot, and 2) there is a quasi-working flip book towards the end of the post that may be of some value.\r\nHere’s the rest of the code:\r\n\r\n\r\ndat %>% \r\n  ggplot() +\r\n  geom_point(aes(x = cty, y = hwy, color = cyl_color),\r\n             \r\n  # make the points slightly transparent and\r\n  # just a little bit bigger\r\n   alpha = 0.7, size = 2) +   \r\n  \r\n  # allow the HTML color codes to work as is,\r\n  # cyl_color goes inside aes() because it is\r\n  # an actual variable in the data, but we need\r\n  # scale_color_identity to interpret the column\r\n  # literally, as an html color code.\r\n  scale_color_identity() + \r\n  \r\n  # facet by the column with the HTML/CSS\r\n  facet_wrap(~ cyl_text) +\r\n  \r\n  # notice we are using the variables for\r\n  # title, subtitle and caption from above\r\n  labs(\r\n    title = title,\r\n    subtitle = subtitle,\r\n    x = 'City MPG',\r\n    y = 'Highway\\nMPG',\r\n    caption = caption\r\n  ) +\r\n  \r\n  # applying a minimal theme first, and then\r\n  # we will modify individual theme components\r\n  # I think the order matters? yeah.\r\n  theme_minimal() +\r\n  \r\n  # within theme() we modify individual bits \r\n  # of the entire theme. This is where\r\n  # powerful customization of your ggplot2\r\n  # graphic is possible. Remember, I don't\r\n  # memorize most of this stuff. If you use\r\n  # something a lot, then sure you will start\r\n  # to just know it. But I'm always referencing\r\n  # this: https://ggplot2.tidyverse.org/reference/theme.html\r\n  theme(\r\n    \r\n    # set the font for all of the text on the\r\n    # plot to 'Fira Code' (the font we add\r\n    # up above from google fonts)\r\n    text = element_text(family = 'Fira Code'),\r\n    \r\n    # hide the individual facet panels\r\n    panel.grid = element_blank(),\r\n    \r\n    # increase the space between the individual\r\n    # facet panels\r\n    panel.spacing = unit(1, \"lines\"),\r\n    \r\n    # change the entire plot background to the grayish\r\n    # color that you see in the plot\r\n    plot.background = element_rect(fill = '#D3D3D3'),\r\n    \r\n    # adjust the margin, or white space, around the plot,\r\n    # I didn't want labels and things to look too squished\r\n    # # c(top, right, bottom, left)\r\n    plot.margin = unit(c(.5,.75,.5,.75), 'cm'),\r\n    \r\n    # left align the plot's title to the edge of the \r\n    # entire graphic, people seem to think this looks\r\n    # better and I tend to agree\r\n    plot.title.position = 'plot',\r\n    \r\n    # remember that element_markdown is from ggtext,\r\n    # in addition to allowing the html/css to render\r\n    # correctly, we are also adjusting the size, face,\r\n    # color, and lineheight of the title\r\n    plot.title = element_markdown(size = 16,\r\n                                  face = 'bold',\r\n                                  color = '#525252',\r\n                                  lineheight = 1.2),\r\n    \r\n    # element_textbox_simple is again from ggtext,\r\n    # which creates the text box you see surrounding the\r\n    # subtitle. Again we are adjusting a few things,\r\n    # like the padding or spacing around the insight of\r\n    # the text box. I also kind of like the darker\r\n    # color just for the box.\r\n    plot.subtitle = element_textbox_simple(\r\n        size = 10,\r\n        lineheight = 1.2,\r\n        padding = margin(5.5, 5.5, 5.5, 5.5),\r\n        margin = margin(0, 0, 5.5, 0),\r\n        fill = '#CCCCCC'),\r\n    \r\n    # I think I used the box here just to make it\r\n    # easier to customize the caption. I am sure\r\n    # there are other ways to do this.\r\n    plot.caption = element_textbox_simple(\r\n            size = 9,\r\n            lineheight = 1.2,\r\n            margin = margin(20, 50, 0, 0),\r\n            fill = '#D3D3D3'),\r\n    \r\n    # adjusting the axis texts and titles,\r\n    # again modifying the margin slightly\r\n    # (top, right, bottom, left) to increase\r\n    # the space between the plot and the x axis\r\n    # title. The axis.title.y line adjusts the\r\n    # y axis title so that you don't have to title\r\n    # your head to the side to read the label.\r\n    axis.text = element_text(size = 12),\r\n    axis.title = element_text(size = 12, face = 'bold', color = '#525252'),\r\n    axis.title.x = element_text(margin = unit(c(.5, 0, 0, 0), \"cm\")),\r\n    axis.title.y = element_text(angle = 0, vjust = 1),\r\n    \r\n    # increase the size of the facet labels\r\n    strip.text = element_markdown(size = 14)\r\n    )\r\n\r\n\r\n\r\n\r\nHere’s a semi functional flipbook\r\nThis probably only looks decent on desktop, and even then I’m not so sure. Click on the arrows to go forward or backward in the code.\r\n\r\n\r\n\r\nfitvids('.shareagain', {players: 'iframe'});\r\n\r\nThanks so much for reading! I hope you found this post helpful. Go forth and customize your ggplot2 graphics for better communication! Feel free to leave any questions or comments below, or shoot me an email at avery@codewithavery.com. I’m always open to feedback and happy to help anyone with their coding questions.\r\nR and package versions\r\n\r\n\r\nR.version\r\n\r\n\r\n               _                           \r\nplatform       x86_64-w64-mingw32          \r\narch           x86_64                      \r\nos             mingw32                     \r\nsystem         x86_64, mingw32             \r\nstatus                                     \r\nmajor          4                           \r\nminor          0.3                         \r\nyear           2020                        \r\nmonth          10                          \r\nday            10                          \r\nsvn rev        79318                       \r\nlanguage       R                           \r\nversion.string R version 4.0.3 (2020-10-10)\r\nnickname       Bunny-Wunnies Freak Out     \r\n\r\npackageVersion('dplyr')\r\n\r\n\r\n[1] '1.0.4'\r\n\r\npackageVersion('ggplot2')\r\n\r\n\r\n[1] '3.3.3'\r\n\r\npackageVersion('ggtext')\r\n\r\n\r\n[1] '0.1.1'\r\n\r\npackageVersion('showtext')\r\n\r\n\r\n[1] '0.9.2'\r\n\r\npackageVersion('glue')\r\n\r\n\r\n[1] '1.4.2'\r\n\r\n\r\n\r\n\r\n",
    "preview": "posts/2021-02-22-ggtext/../../img/2021/2021-02-22-ggtext/plot_standarized_tests.png",
    "last_modified": "2021-02-23T12:30:44-07:00",
    "input_file": {},
    "preview_width": 917,
    "preview_height": 480
  },
  {
    "path": "posts/2020-10-06-string-interpolation/",
    "title": "String interpolation in Python and R (the best ways)",
    "description": "What are the different ways of executing code within a string of text in Python and R? This post looks into the good and arguably best forms of string interpolation in two of the most popular programming languages for data science.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-10-06",
    "categories": [],
    "contents": "\r\nHere we go\r\nIn layman’s terms, string interpolation is executing code within a string (text).\r\nLet’s keep this post short and to the point. We’ll look at some R code, then move on to Python. I’ll simply show how to use each method of string interpolation, and highlight my preferred method for each language.\r\nString interpolation in R\r\nGood\r\npaste is a good way to paste together text and variables, although not my favorite.\r\n\r\n\r\nname <- 'Avery'\r\nage <- 24\r\npaste('Hello! My name is', name, 'and I am', age, 'years old.')\r\n\r\n\r\n[1] \"Hello! My name is Avery and I am 24 years old.\"\r\n\r\nRemember that R is vectorized, so no need for a for loop in cases like this:\r\n\r\n\r\nname <- c('Avery', 'Susan', 'Joe')\r\nage <- c(24, 20, 40)\r\npaste('Hello! My name is', name, 'and I am', age, 'years old.')\r\n\r\n\r\n[1] \"Hello! My name is Avery and I am 24 years old.\"\r\n[2] \"Hello! My name is Susan and I am 20 years old.\"\r\n[3] \"Hello! My name is Joe and I am 40 years old.\"  \r\n\r\nThe default separator in paste is a space \" \", but obviously you can change that to something else.\r\n\r\n\r\nx <- 25\r\ny <- 15\r\npaste('x + y', x + y, sep = ' = ')\r\n\r\n\r\n[1] \"x + y = 40\"\r\n\r\nRun ?paste for more information.\r\nBest\r\npaste is good, but glue is best. Ever since I discovered the glue function from the glue package, I rarely use paste anymore.\r\nDon’t forget to load the package:\r\n\r\n\r\nlibrary(glue) # package for easy string interpolation\r\n\r\n\r\n\r\nglue is easy to use. Just put code that you want to execute inside of braces { }. Also, everything goes inside of quotes.\r\n\r\n\r\nsize <- c(\"Small\", \"Medium\", \"Large\")\r\ncyls <- sort(unique(mtcars$cyl)) # mtcars is a built-in dataset that comes with R\r\n\r\nglue(\"{size} cars sometimes have {cyls} cylinders. But don't quote me, I'm not a car guy.\")\r\n\r\n\r\nSmall cars sometimes have 4 cylinders. But don't quote me, I'm not a car guy.\r\nMedium cars sometimes have 6 cylinders. But don't quote me, I'm not a car guy.\r\nLarge cars sometimes have 8 cylinders. But don't quote me, I'm not a car guy.\r\n\r\nPersonally, I find the glue { } syntax cleaner, easier to read and type, and more intuitive than the base R paste. For tidyverse users, glue style syntax is also popping up in other places in the tidyverse (for example, see the .names argument in the relatively new dplyr::across function).\r\nString interpolation in Python\r\n\r\n\r\nlibrary(reticulate) # package for running Python within R\r\n\r\n\r\n\r\nGood\r\nSimilar to R’s paste:\r\n\r\nname = 'Avery'\r\nage = 24\r\nprint('Hello! My name is ' + name + ' and I am ' + str(age) + ' years old!')\r\nHello! My name is Avery and I am 24 years old!\r\n\r\nThis method is also pretty clunky. Let’s try something better.\r\nBetter\r\nUsing the format method is not too shabby. Things are starting to look like R’s glue.\r\n\r\nprint('Hello! My name is {name} and I am {age} years old!'.format(name = name, age = age))\r\nHello! My name is Avery and I am 24 years old!\r\n\r\nNotice above how we specify name = name inside of the format method. The placeholders don’t actually represent our variables like you might think. You, the programmer, have to specify placeholder = some_variable. You also don’t have to put anything inside of the {}. If you leave the curly braces empty, Python relies on the order of the arguments that you put inside of the format method.\r\n\r\nemotion = 'sad'\r\nprint('I am sick and tired of {}! I am so {}.'.format('Covid', emotion))\r\nI am sick and tired of Covid! I am so sad.\r\n\r\nformat works fine, but I think Python really knocks it out of the park with something called f-strings.\r\nBest\r\nThe syntax is almost exactly the same as glue. Instead of writing glue('some text {code}'), you just add the letter f before any string. This allows you to use the same curly brace syntax as before, easily executing the code within.\r\n\r\nlanguage = 'French'\r\ntime = '3 years'\r\n\r\nprint(f'I have been speaking {language} for about {time}. I feel accomplished.')\r\nI have been speaking French for about 3 years. I feel accomplished.\r\n\r\nCareful though. Python isn’t vectorized like R is, so the following code might not work as expected.\r\n\r\nlanguages = ['French', 'Spanish', 'English']\r\ntimes = ['3 years', '1 year', 'my entire life'] \r\n\r\nprint(f'I have been speaking {languages} for {times}. I feel accomplished.')\r\nI have been speaking ['French', 'Spanish', 'English'] for ['3 years', '1 year', 'my entire life']. I feel accomplished.\r\n\r\nYou have to do more work, which isn’t too terrible.\r\n\r\nfor (l, t) in zip(languages, times):\r\n  print(f'I have been speaking {l} for {t}. I feel accomplished.')\r\nI have been speaking French for 3 years. I feel accomplished.\r\nI have been speaking Spanish for 1 year. I feel accomplished.\r\nI have been speaking English for my entire life. I feel accomplished.\r\n\r\nMany experienced programmers would say that if you are using a for loop, you probably shouldn’t be. There is usually a better option. Loops in generally are very error prone. Its probably not apparent with this toy example, but in case you were curious here is the same thing as above accomplished with map and a lambda function.\r\n\r\nlist(\r\n  map(\r\n    lambda l, t: print(f'I have been speaking {l} for {t}. I feel accomplished.'),\r\n    languages, times\r\n    )\r\n  )\r\nI have been speaking French for 3 years. I feel accomplished.\r\nI have been speaking Spanish for 1 year. I feel accomplished.\r\nI have been speaking English for my entire life. I feel accomplished.\r\n[None, None, None]\r\n\r\nI won’t get into map and lambda here, but there are tons of great resources our there on the web. If you don’t understand the code above, just google “python map and lambda.”\r\nThat’s all for now folks\r\nLike I said, short and to the point. If you learned something here, especially if you didn’t know about glue and f-strings and you think they are useful, well then that is awesome. Thanks for reading. Stay safe and happy coding!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-21T00:02:17-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-24-fizzbuzz/",
    "title": "Solving the FizzBuzz problem in R and Python",
    "description": "Small practice problems can be fun and challenging ways to test your programming skills! Solutions in R and Python to the FizzBuzz problem are provided at the end.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-08-24",
    "categories": [],
    "contents": "\r\nWhy FizzBuzz and friends?\r\nThe FizzBuzz problem is one of those classic programming problems that is awesome for beginners of pretty much any language. Even though it is rather simple, it can also be a good refresher on some basic programming concepts.\r\nHere is the actual problem:\r\nWrite a short program that prints each number from 1 to 100 on a new line. For each multiple of 3, print “Fizz” instead of the number. For each multiple of 5, print “Buzz” instead of the number. For numbers which are multiples of both 3 and 5, print “FizzBuzz” instead of the number.\r\nWhy is it a good problem for beginners?\r\nThe problem focuses on many fundamentals that are ubiquitous in just about any programming language:\r\nconditionals\r\nfunctions\r\nbooleans\r\ndata types\r\noperators\r\niteration\r\nThere are also different ways to arrive at the same answer, which I would argue should be viewed as a good thing.\r\nWhen learning the ropes of a new programming language, trying to solve problems like the FizzBuzz problem can help you gauge where you are at in your understanding of the basics. That being said, the best way to learn new skills is by working on real world projects. Whether that means wrangling and exploring messy data with R, or developing a basic game with Python, the idea is to practice what you are learning as quickly as you can. Here is an example of a small project that I worked on to improve my web scraping skills.\r\nDon’t fall down the rabbit hole of never ending videos or tutorials without applying what you learn towards something bigger. Establish a solid foundation of working with real data, real projects, and real people. It’s not MOOCs all the way down.\r\nSmall practice problems and personal projects can be both fun and challenging. And remember, googling is allowed! While having a solid understanding and ability of the basics is critical, memorizing tons and tons of code is not worth your time. Allow Google and Stack Overflow to become your best friends. Learning how to learn and effectively solve problems is much more important than rote memorization of tons of code.\r\nSTOP! I mean, stop if you want to. If you scroll down more, you will see solutions to the FizzBuzz problem in both R and Python. I would encourage you to try to solve the problem yourself before continuing. Choose R or Python, or both (or something else)!\r\nToo easy? Time yourself. If you are multilingual, you could even solve the problem in multiple languages and see how long it takes you in each. Have fun with it.\r\nI will probably place an ad or two here in order to put more space in between you and the solutions.\r\nhackerearth is a popular place to find more programming problems.\r\nCodeChef and w3schools are also good resources for practice and learning.\r\nFizzBuzz solutions\r\nKeep in mind, there are multiple roads that lead to the same destination. What road did you take?\r\nR solution\r\nHere is one way to do it in R:\r\n\r\n\r\nfizz_buzz <- function(num) {\r\n    if (num %% 3 == 0 & num %% 5 == 0) {\r\n        print(\"FizzBuzz\")\r\n    } else if (num %% 3 == 0) {\r\n        print(\"Fizz\")\r\n    } else if (num %% 5 == 0) {\r\n        print(\"Buzz\")\r\n    } else {\r\n        print(num)\r\n    }\r\n}\r\n\r\npurrr::walk(1:100, fizz_buzz)\r\n\r\n\r\n[1] 1\r\n[1] 2\r\n[1] \"Fizz\"\r\n[1] 4\r\n[1] \"Buzz\"\r\n[1] \"Fizz\"\r\n[1] 7\r\n[1] 8\r\n[1] \"Fizz\"\r\n[1] \"Buzz\"\r\n[1] 11\r\n[1] \"Fizz\"\r\n[1] 13\r\n[1] 14\r\n[1] \"FizzBuzz\"\r\n[1] 16\r\n[1] 17\r\n[1] \"Fizz\"\r\n[1] 19\r\n[1] \"Buzz\"\r\n[1] \"Fizz\"\r\n[1] 22\r\n[1] 23\r\n[1] \"Fizz\"\r\n[1] \"Buzz\"\r\n[1] 26\r\n[1] \"Fizz\"\r\n[1] 28\r\n[1] 29\r\n[1] \"FizzBuzz\"\r\n[1] 31\r\n[1] 32\r\n[1] \"Fizz\"\r\n[1] 34\r\n[1] \"Buzz\"\r\n[1] \"Fizz\"\r\n[1] 37\r\n[1] 38\r\n[1] \"Fizz\"\r\n[1] \"Buzz\"\r\n[1] 41\r\n[1] \"Fizz\"\r\n[1] 43\r\n[1] 44\r\n[1] \"FizzBuzz\"\r\n[1] 46\r\n[1] 47\r\n[1] \"Fizz\"\r\n[1] 49\r\n[1] \"Buzz\"\r\n[1] \"Fizz\"\r\n[1] 52\r\n[1] 53\r\n[1] \"Fizz\"\r\n[1] \"Buzz\"\r\n[1] 56\r\n[1] \"Fizz\"\r\n[1] 58\r\n[1] 59\r\n[1] \"FizzBuzz\"\r\n[1] 61\r\n[1] 62\r\n[1] \"Fizz\"\r\n[1] 64\r\n[1] \"Buzz\"\r\n[1] \"Fizz\"\r\n[1] 67\r\n[1] 68\r\n[1] \"Fizz\"\r\n[1] \"Buzz\"\r\n[1] 71\r\n[1] \"Fizz\"\r\n[1] 73\r\n[1] 74\r\n[1] \"FizzBuzz\"\r\n[1] 76\r\n[1] 77\r\n[1] \"Fizz\"\r\n[1] 79\r\n[1] \"Buzz\"\r\n[1] \"Fizz\"\r\n[1] 82\r\n[1] 83\r\n[1] \"Fizz\"\r\n[1] \"Buzz\"\r\n[1] 86\r\n[1] \"Fizz\"\r\n[1] 88\r\n[1] 89\r\n[1] \"FizzBuzz\"\r\n[1] 91\r\n[1] 92\r\n[1] \"Fizz\"\r\n[1] 94\r\n[1] \"Buzz\"\r\n[1] \"Fizz\"\r\n[1] 97\r\n[1] 98\r\n[1] \"Fizz\"\r\n[1] \"Buzz\"\r\n\r\nPython solution\r\nHere is one way to do it in Python:\r\n\r\ndef fizzbuzz(num):\r\n    if num % 3 == 0 and num % 5 == 0:\r\n        return(\"FizzBuzz\")\r\n    elif num % 3 == 0:\r\n        return(\"Fizz\")\r\n    elif num % 5 == 0:\r\n        return(\"Buzz\")\r\n    else:\r\n        return(num)\r\n        \r\nnumbers = list(range(1, 101))\r\n\r\nfor number in numbers:\r\n    print(fizzbuzz(number))\r\n1\r\n2\r\nFizz\r\n4\r\nBuzz\r\nFizz\r\n7\r\n8\r\nFizz\r\nBuzz\r\n11\r\nFizz\r\n13\r\n14\r\nFizzBuzz\r\n16\r\n17\r\nFizz\r\n19\r\nBuzz\r\nFizz\r\n22\r\n23\r\nFizz\r\nBuzz\r\n26\r\nFizz\r\n28\r\n29\r\nFizzBuzz\r\n31\r\n32\r\nFizz\r\n34\r\nBuzz\r\nFizz\r\n37\r\n38\r\nFizz\r\nBuzz\r\n41\r\nFizz\r\n43\r\n44\r\nFizzBuzz\r\n46\r\n47\r\nFizz\r\n49\r\nBuzz\r\nFizz\r\n52\r\n53\r\nFizz\r\nBuzz\r\n56\r\nFizz\r\n58\r\n59\r\nFizzBuzz\r\n61\r\n62\r\nFizz\r\n64\r\nBuzz\r\nFizz\r\n67\r\n68\r\nFizz\r\nBuzz\r\n71\r\nFizz\r\n73\r\n74\r\nFizzBuzz\r\n76\r\n77\r\nFizz\r\n79\r\nBuzz\r\nFizz\r\n82\r\n83\r\nFizz\r\nBuzz\r\n86\r\nFizz\r\n88\r\n89\r\nFizzBuzz\r\n91\r\n92\r\nFizz\r\n94\r\nBuzz\r\nFizz\r\n97\r\n98\r\nFizz\r\nBuzz\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-20T23:56:18-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-12-tidy-tuesday-avatar/",
    "title": "The greatest Avatar visualizations you have ever seen",
    "description": "Showcasing the best Avatar: The Last Airbender visualizations from the Tidy Tuesday weekly challenge. All graphics are from the amazing R community!",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-08-12",
    "categories": [],
    "contents": "\r\nWelcome\r\nWelcome lovers of Avatar: The Last Airbender, users of R, and all humans!\r\nIf you are reading this, but are not from any of the three communities mentioned above, then I fear that humanity has been replaced by some sort of general artificial intelligence.\r\n\r\nBackstory\r\n\r\nOne thing that I love about being an R programmer is the community. Most people are supportive and generous with their time and talents. The #rstats Twitter-sphere is active and helpful, and there are thousands of answers to R related questions on stack overflow. There exist many insightful blogs, and there are even local meetups in different areas for R users to teach and learn together.\r\nTo make a long story short, the community is awesome.\r\nA fair amount of the community, both beginners and seasoned R users, participate in the weekly data project, Tidy Tuesday. Each week a dataset is chosen for exploration and analysis. According to the Tidy Tuesday README, “the intent of Tidy Tuesday is to provide a safe and supportive forum for individuals to practice their wrangling and data visualization skills…” Results are often shared via Twitter.\r\nOn Monday of this week, I saw on Twitter that my R package, appa, was being used for this week’s Tidy Tuesday dataset! This was a crazy surprise for a little R programmer such as myself. appa is an incredibly simple package that just contains one thing: a dataset containing Avatar: The Last Airbender transcript, writer & director, and IMDB ratings data. I scraped most of this data from the Avatar Wiki, which journey I describe in this post and subsequent exploration I describe here.\r\nMy point in telling you all of this is the following: if you have a personal project or goal that you want to accomplish, do it! If there is a specific skill that you want to improve upon, practice! You never know what could happen. Last month, I took an informal inventory of my R skills, and decided that I wanted to get better at web scraping. I spent a few hours on a weekend practicing on data that was fun and interesting to me personally, and then I bundled it all up into an easily accessible R package for others to use. I learned a lot, and described my efforts here on the blog in hopes of helping others learn something as well.\r\nOne month later, someone from the community recommends the data be used for Tidy Tuesday. Turns out a lot of people who program with R also love Avatar! The response from the community was epic! The week isn’t even finished yet, but there are already so many incredible plots and amazing works of art. And now here they are for your viewing pleasure.\r\nGallery\r\nEach of these visualizations was created by members of the R community as part of the 2020-08-11 Tidy Tuesday challenge. Full credit for such amazing work is given to them, and where possible, a link to the source code is provided (a few individuals did not include a link to there source code in their tweet).\r\n\r\n\r\n\r\nsource\r\nsource\r\n source code unavailable\r\nsource\r\nsource\r\nsource\r\n source code unavailable, plot created by Jacqueline Nolis - @skyetetra\r\nsource\r\n source code unavailable, plot created by Alex Vigderman - @VigManOnCampus\r\n source code unavailable, plot created by Dizz - @DataDizz\r\n source code unavailable, plot created by Colin Walder - @bonschorno\r\nsource\r\n source code unavailable, plot created by Cedric Scherer - @CedScherer\r\nsource\r\nsource\r\n\r\n\r\n\r\n\r\nsource\r\nsource\r\nPeople do awesome things! Hope you enjoyed this!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-20T23:50:50-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-08-08-covid/",
    "title": "Covid... and why 3D plots are bad",
    "description": "Comparing the death-to-case ratio between the US and Sweden, and a brief commentary on 3D plots.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-08-08",
    "categories": [],
    "contents": "\r\n\r\nSo Covid is actually dangerous to Grandma\r\nAccording to CDC data (current as of 2020/07/16), almost 1 out of 3 people over 80 years old who contracted Covid 19 in the US died.\r\nHold on. There are some major problems with the above statement. The truth is the data that’s available just isn’t high quality. I researched multiple data sources in preparation for this post, and the actual numbers vary. We can’t know the actual number of positive cases in the US. We have a huge population of over 330 million people. False positives and false negatives exist, and there are certainly inviduals who have had Covid who haven’t been tested.\r\nFurthermore, there are Covid reported deaths that are reported inaccurately, or that involve other factors, such as pre-exisiting conditions or illnesses. My wife just taught me a new word, comorbidity: the simultaneous presence of two or more chronic diseases or conditions in a patient. Sadly, someone can be dying of cancer, contract Covid in the eleventh hour, and then they will pass away and be counted as a Covid death. Seems sketchy.\r\nWith all that being said, even if we don’t know the exact numbers, we can see trends that suggest that Covid is indeed more dangerous to the elderly. The opposite also seems to be true: Covid 19 really isn’t that harmful to young people.\r\nLooking at the table below, if we believe that this data is even somewhat accurate, we can see that there is extremely low risk for individuals under 40 years old. There is also very low risk even for persons in their 40s and 50s. Lastly, it is clear that individuals 70 years of age and older are at great risk.\r\n\r\nTable 1: Data current as of 2020/07/16\r\nage_group\r\ndeaths\r\ncases\r\ndeath_to_case_ratio\r\ncountry\r\n0 - 39\r\n1986\r\n1148160\r\n0.0017297\r\nUSA\r\n40 - 59\r\n12708\r\n856343\r\n0.0148398\r\nUSA\r\n60 - 69\r\n18979\r\n307648\r\n0.0616906\r\nUSA\r\n70 - 79\r\n27449\r\n175515\r\n0.1563912\r\nUSA\r\n80+\r\n52057\r\n177459\r\n0.2933466\r\nUSA\r\n\r\nHere is the same data, but for Sweden.\r\n\r\nTable 2: Data current as of 2020/08/05\r\nage_group\r\ncases\r\ndeaths\r\ndeath_to_case_ratio\r\ncountry\r\n0 - 39\r\n29062\r\n26\r\n0.0008946\r\nSweden\r\n40 - 59\r\n28430\r\n205\r\n0.0072107\r\nSweden\r\n60 - 69\r\n8442\r\n398\r\n0.0471452\r\nSweden\r\n70 - 79\r\n5917\r\n1243\r\n0.2100727\r\nSweden\r\n80+\r\n10098\r\n3894\r\n0.3856209\r\nSweden\r\n\r\nWhy Sweden?\r\nWhy am I comparing the US and Sweden? My father sent me an article commenting on the Western media’s narrative with regards to the “Swedish Public Health Disaster.” Actually, the article was flat out refuting that there was even a public health disaster. This is a very politically charged topic, so I won’t go into it too much. However, the basic argument is about lockdowns for Covid. To lock down or not to lock down, that is the question.\r\nLike I said, I don’t want to get too political, but I also want to try and make sense of the available data. Most of the workforce in the US is relatively young, right? A quick search on the interwebs reports the average age of US workers to be 42.3 years. That means that the vast majority of the working class has little to no risk from Covid, right? What if we allowed people to go to work who wanted to work, and focus most of our efforts on protecting the elderly? I am curious to hear other people’s opinions. Is this view too simplistic?\r\nAt least from the death-to-case ratio that I plotted, we can see that Sweden did worse than the US for person’s over 70 years, whereas Sweden’s younger population was less affected than that of the US. As you’ll see in the next plot, Sweden does much better in terms of the mortality rate.\r\n3D is almost never the answer\r\nOkay, changing gears. I want to make a comment on this graph from that article I mentioned, instead of going on about Covid. As a clarification, the author of that article was comparing the mortality rate, while I was comparing the death-to-case ratio, or the case fatality ratio. They are different. From what I understand, mortality rate uses some form of the population as the denominator, whereas I used cases.\r\nNow, look at the plot below. It does not look like Sweden had much of a public health disaster as a result of not locking down. That’s just what the mortality rate shows. I wonder if the media every lies to us for their own political gain? Dang it, I am starting to get political. No more Covid talk. Let’s critique this plot.\r\n\r\nWhy the heck is it in 3D??? We have the age groups represented along the x-axis, the mortality rate represented along the y-axis, and the countries encoded by color. Sure that’s three variables, but we successfully represented the third variable, country, by color. Is there another variable along the z-axis? Nope. It looks like 3D was added to this plot just to be “fancy.” That is a mistake. Not to bash on the author of this plot, but this is a fine example of making something more complicated than it needs to be. It is more difficult to compare the countrys’ mortality rates using the bars with the unnecessary added dimension than it would be to simply compare the height of the bars if they were in 2D.\r\nAvoid 3D plots when they make something more complicated than it needs to be, which they usually do.\r\nHumans are most accurate at decoding data from visualizations when comparing position along a common scale. Just look at my original plot again.\r\nStacked anything is also often harder to read. Which graph makes it easier to see the difference between the US and Sweden?\r\n\r\nIt is easy to compare the height of one bar to the other when standing side by side, but much more difficult to compare the two when stacked or in 3D. ## Notes\r\nA couple of disclaimers. This post isn’t meant to be super political. On a personal level, I wanted to take a closer look at the available data on Covid 19 because there is a lot of misinformation. There is a lot of Covid data out there, but the actual numbers vary, and the data quality isn’t always the best. I don’t guarantee that the stats here are 100% correct either, but rather they should give an accurate display of general trends that appear in the available data.\r\nNotice that I am not calculating the mortality rate, but instead the death-to-case ratio. Here is the definition of that ratio taken directly from the CDC website:\r\nThe death-to-case ratio is the number of deaths attributed to a particular disease during a specified time period divided by the number of new cases of that disease identified during the same time period. The death-to-case ratio is a ratio but not necessarily a proportion, because some of the deaths that are counted in the numerator might have occurred among persons who developed disease in an earlier period, and are therefore not counted in the denominator.\r\nSources\r\nI found the US CDC data here and the Swedish data here. The Swedish data was in… swedish, so I whipped out google translate and changed the column names quickly in excel.\r\nMinor data munging\r\nRead on if you are interested in any of the data wrangling or R code used in this post. If you are not interested, then farewell and goodbye.\r\nHere are the needed packages.\r\n\r\n\r\nlibrary(tidyverse) # all the things\r\nlibrary(janitor) # clean the column names easily\r\nlibrary(ggthemr) # set up a nice theme for ggplots\r\nlibrary(patchwork) # compose multiple ggplot2 into one \r\n\r\nggthemr(\"flat dark\", type = \"outer\") # theme we set for out gglots\r\n\r\n\r\n\r\nHere is a glimpse of the US data from the CDC website. It isn’t summarized data, but instead each row represents an individual.\r\n\r\n\r\ndat <- read_csv(\"content/post/data/cdc-covid-data.csv\") %>% clean_names()\r\nglimpse(dat)\r\n\r\n# You won't be able to run the code yourself until we read in the data from GitHub below.\r\n\r\n\r\n\r\nRows: 2,668,175\r\nColumns: 11\r\n$ cdc_report_dt               <date> 2020-07-03, 2020-05-27, 2020-06-06, 2020-06-28...\r\n$ pos_spec_dt                 <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...\r\n$ onset_dt                    <date> NA, NA, NA, NA, NA, 2020-05-23, NA, NA, NA, 20...\r\n$ current_status              <chr> \"Laboratory-confirmed case\", \"Laboratory-confir...\r\n$ sex                         <chr> \"Female\", \"Female\", \"Female\", \"Female\", \"Female...\r\n$ age_group                   <chr> \"0 - 9 Years\", \"0 - 9 Years\", \"0 - 9 Years\", \"0...\r\n$ race_and_ethnicity_combined <chr> \"American Indian/Alaska Native, Non-Hispanic\", ...\r\n$ hosp_yn                     <chr> \"Missing\", \"No\", \"Missing\", \"Missing\", \"Missing...\r\n$ icu_yn                      <chr> \"Missing\", \"Missing\", \"Missing\", \"Missing\", \"Mi...\r\n$ death_yn                    <chr> \"Missing\", \"No\", \"Missing\", \"Missing\", \"Missing...\r\n$ medcond_yn                  <chr> \"Missing\", \"Missing\", \"Missing\", \"Missing\", \"Mi...\r\nLet’s tidy up the US data first. Select just the columns we care about.\r\n\r\n\r\ndat_us <- dat %>% select(age_group, current_status, death_yn)\r\n\r\ndat_us %>% count(current_status)\r\n\r\n\r\n\r\n# A tibble: 2 x 2\r\n  current_status                  n\r\n  <chr>                       <int>\r\n1 Laboratory-confirmed case 2552919\r\n2 Probable Case              115256\r\nWe are going to treat lab confirmed and probable case as basically the same. There are also unknown and missing values in the death_yn column, so we’ll clean that up as well.\r\n\r\n\r\ndat_us <- dat_us %>% \r\n    mutate(death = ifelse(death_yn == \"Yes\", \"confirmed_death\", \"no death\")) %>% \r\n    count(age_group, death)\r\n\r\nwrite_csv(dat_us, \"C:/Users/averyrobbins1/Desktop/robbins-data/covid-data/cdc-covid-data.csv\")\r\n\r\n\r\n\r\nThe CDC data wasn’t summarized initially, as I said before, so I did some quick data wrangling to get it in the desired format. This had to be done because the original data was too large for GitHub. Any R users can actually follow along and run code from this point on.\r\n\r\n\r\ndat_us <- read_csv(\"https://github.com/averyrobbins1/robbins-data/blob/master/covid-data/cdc-covid-data.csv?raw=true\")\r\n\r\nglimpse(dat_us)\r\n\r\n\r\nRows: 22\r\nColumns: 3\r\n$ age_group <chr> \"0 - 9 Years\", \"0 - 9 Years\", \"10 - 19 Years\", ...\r\n$ death     <chr> \"confirmed_death\", \"no death\", \"confirmed_death...\r\n$ n         <dbl> 33, 67667, 54, 161298, 498, 467973, 1401, 44923...\r\n\r\nAnd here is the Swedish data from the Public Health Agency of Sweden. Notice that it is already summarized.\r\n\r\n\r\ndat_swed <- read_csv(\"https://github.com/averyrobbins1/robbins-data/blob/master/covid-data/swedish-covid-data.csv?raw=true\") %>% clean_names()\r\n\r\nglimpse(dat_swed)\r\n\r\n\r\nRows: 11\r\nColumns: 4\r\n$ age_group                            <chr> \"0_9\", \"10_19\", \"20_...\r\n$ total_number_of_cases                <dbl> 522, 3440, 12465, 12...\r\n$ total_number_of_intensive_care_units <dbl> 8, 15, 92, 116, 282,...\r\n$ total_number_of_deaths               <dbl> 1, 0, 9, 16, 44, 161...\r\n\r\nKind of annoying, but the CDC data was only current as of 2020/07/16. The Swedish data was current as of 2020-08-05, but, because it was already summarized by age group, I didn’t see a good way to filter the data up to 2020/07/16. The daily death and case counts didn’t have age information for some reason. Since the deaths and cases in Sweden were so few after 2020/07/16, it shouldn’t affect the overall picture.\r\nWe’ll need to do some more wrangling to get the data how we want it for the graph.\r\n\r\n\r\ndat_us <- dat_us %>% mutate(age_group = str_remove(age_group, \" Years\"))\r\ndat_us %>% count(age_group)\r\n\r\n\r\n# A tibble: 11 x 2\r\n   age_group     n\r\n * <chr>     <int>\r\n 1 0 - 9         2\r\n 2 10 - 19       2\r\n 3 20 - 29       2\r\n 4 30 - 39       2\r\n 5 40 - 49       2\r\n 6 50 - 59       2\r\n 7 60 - 69       2\r\n 8 70 - 79       2\r\n 9 80+           2\r\n10 Unknown       2\r\n11 <NA>          2\r\n\r\nWe’ll filter out the Unknown and NA values since they won’t do us much good.\r\n\r\n\r\ndat_us <- dat_us %>% filter(age_group != \"Unknown\", !is.na(age_group))\r\n\r\n\r\n\r\nNext, we will consolidate the age groups a bit more, and calculate the death-to-case ratio.\r\n\r\n\r\ndat_us2 <- dat_us %>%\r\n  mutate(\r\n    age_group = case_when(\r\n      age_group %in% c(\"0 - 9\", \"10 - 19\", \"20 - 29\", \"30 - 39\") ~ \"0 - 39\",\r\n      age_group %in% c(\"40 - 49\", \"50 - 59\")                     ~ \"40 - 59\",\r\n      TRUE                                                       ~ age_group\r\n    )\r\n  ) %>% \r\n  group_by(age_group, death) %>% \r\n  summarize(n = sum(n)) %>% \r\n  mutate(cases = sum(n)) %>% \r\n  filter(death != \"no death\") %>% \r\n  transmute(age_group, deaths = n,  cases,\r\n            death_to_case_ratio = (deaths/cases),\r\n            country = \"USA\") %>% \r\n  ungroup()\r\n\r\ndat_us2 %>% knitr::kable()\r\n\r\n\r\nage_group\r\ndeaths\r\ncases\r\ndeath_to_case_ratio\r\ncountry\r\n0 - 39\r\n1986\r\n1148160\r\n0.0017297\r\nUSA\r\n40 - 59\r\n12708\r\n856343\r\n0.0148398\r\nUSA\r\n60 - 69\r\n18979\r\n307648\r\n0.0616906\r\nUSA\r\n70 - 79\r\n27449\r\n175515\r\n0.1563912\r\nUSA\r\n80+\r\n52057\r\n177459\r\n0.2933466\r\nUSA\r\n\r\nWe’ll also do some wrangling to get the Swedish data how we need it. A lot of the same. Clean up the age_group column, consolidate the groups a bit more, and calculate the death-to-case ratio.\r\n\r\n\r\ndat_swed2 <- dat_swed %>% \r\n  mutate(age_group = str_replace(age_group, \"_\", \" - \"),\r\n           age_group = ifelse(age_group == \"80 - 89\" | age_group == \"90 - plus\",\r\n                              \"80+\", age_group)) %>% \r\n  filter(!is.na(age_group)) %>% \r\n  select(age_group, cases = total_number_of_cases, deaths = total_number_of_deaths) %>%\r\n  mutate(\r\n    age_group = case_when(\r\n      age_group %in% c(\"0 - 9\", \"10 - 19\", \"20 - 29\", \"30 - 39\") ~ \"0 - 39\",\r\n      age_group %in% c(\"40 - 49\", \"50 - 59\")                     ~ \"40 - 59\",\r\n      TRUE                                                       ~ age_group\r\n    )\r\n  ) %>% \r\n  group_by(age_group) %>%\r\n  summarize(cases = sum(cases), deaths = sum(deaths)) %>% \r\n  mutate(death_to_case_ratio = (deaths/cases),\r\n         country = \"Sweden\")\r\n\r\ndat_swed2 %>% knitr::kable()\r\n\r\n\r\nage_group\r\ncases\r\ndeaths\r\ndeath_to_case_ratio\r\ncountry\r\n0 - 39\r\n29062\r\n26\r\n0.0008946\r\nSweden\r\n40 - 59\r\n28430\r\n205\r\n0.0072107\r\nSweden\r\n60 - 69\r\n8442\r\n398\r\n0.0471452\r\nSweden\r\n70 - 79\r\n5917\r\n1243\r\n0.2100727\r\nSweden\r\n80+\r\n10098\r\n3894\r\n0.3856209\r\nSweden\r\n\r\nHere is the code for the plots:\r\n\r\n\r\nplot1 <- dat_us2 %>% \r\n  bind_rows(dat_swed2) %>% \r\n  ggplot(mapping = aes(x = age_group, y = death_to_case_ratio, fill = country)) +\r\n  geom_col(position = position_dodge2(preserve = \"single\")) +\r\n  labs(title = \"Covid 19 death-to-case ratio\",\r\n       x = \"Age Group\",\r\n       y = NULL,\r\n       fill = \"Country\") +\r\n  theme(text = element_text(family = \"Comic Sans MS\"))\r\n\r\nplot2 <- dat_us2 %>% \r\n  bind_rows(dat_swed2) %>% \r\n  ggplot(mapping = aes(x = age_group, y = death_to_case_ratio, fill = country)) +\r\n  geom_col() +\r\n  labs(title = \"Covid 19 death-to-case ratio\",\r\n       x = \"Age Group\",\r\n       y = NULL,\r\n       fill = \"Country\") +\r\n  theme(text = element_text(family = \"Comic Sans MS\"))\r\n\r\nplot1 + plot2\r\n\r\n\r\n\r\n\r\nThanks\r\nThat’s it! Thanks for reading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-08-08-covid/covid_files/figure-html5/unnamed-chunk-13-1.png",
    "last_modified": "2021-02-20T23:43:49-07:00",
    "input_file": {},
    "preview_width": 1152,
    "preview_height": 576
  },
  {
    "path": "posts/2020-07-30-the-office/",
    "title": "Bears. Beats. Analyzing The Office",
    "description": "A brief, informal analysis of the hilarious sitcom, The Office.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-07-30",
    "categories": [],
    "contents": "\r\nWelcome!\r\nIf this is your first time reading my blog, then welcome! If this is not your first time, then welcome back! The website is still pretty new, but it is awesome to see so many people take an interest in what I have been sharing! If you find this informal analysis of The Office entertaining, then please share it with a friend!\r\nAs a side note, all of the graphs you will see were created using the R programming language, and the data used is from the schrute R package.\r\nMotivation\r\nTo be honest, when I saw the image below for the first time a little voice in my head whispered, “satchel!” Is that sad? Haha! True fans of The Office will understand. A couple weeks ago I analyzed transcript data for Avatar: The Last Airbender, and this time around I thought it would be fun to take a closer at another of my favorite TV shows. So, put on your Gettysburg hats and let’s dive right in.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nHow many times do you think Michael Scott uttered his famous line - “That’s what she said?”\r\nAccording to the transcript data - 23 times! Some others joined in as well, but it was truly Michael’s joke. Am I missing anyone? I did notice that the group “That’s what she said!” from Beach Games was missing from the data.\r\n\r\n\r\n\r\n\r\nWho doesn’t love the world’s best boss, Michael Scott? After season one, of course. The IMDB ratings definitely support that Michael was well loved by many. In the entire series, seasons three and four had the highest average (median) ratings, while the ratings suffered quite a bit after Michael left.\r\n\r\n\r\n\r\n\r\nAdmittedly, this next graph is pretty random, but I wanted to highlight both Michael’s and Toby’s “goodbye” episodes. Toby’s was rated fairly high, but Michael was such an icon that his was one of the highest rated episodes of the series.\r\n\r\n\r\n\r\n\r\n\r\nThroughout the entire series, there are at least eight different managers of the Scranton branch (not counting Robert California’s few seconds before he became CEO). Of the managers who were not Michael Scott, Creed was definitely my favorite:\r\n“It’s a beautiful morning at Dunder-Mifflin, or as I like to call it, Great Bratton. Keep it running.” throws keys - Creed\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nNow let’s look at the entire series and identify when each manager was manager.\r\n\r\n\r\n\r\nA good way of figuring out who was the most liked branch manager of Dunder Mifflin, Scranton, would be to take a poll asking the question: “Who was your favorite Scranton branch manager in The Office?” I would guess the favorite would turn out to be Michael. Another way to gain some insight into the likability of specific managers would be to look at the episode ratings for each episode that they were manager. This is likely flawed because 1) some characters were managers during season finales (or a series finale in Dwight’s case), and those episodes tend to rate higher than others, and 2) there is a large difference between number of episodes as manager for each character.\r\nEither way, this method will just have to do. Using episode ratings certainly supports something that I agree with: Nellie was the worst branch manager! I am sorry, but I just wasn’t a fan of her. Also, I would say that Deangelo is rated too high on this scale, and it is probably because I classified him as manager during the episode, Goodbye, Michael.\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\nOkay, now let’s take a look at important words in each season. One measure of how important a word may be is a simple word count, or term frequency (how frequently a word occurrs in a body of text). Another measure is something called inverse document frequency. This statistic makes very commonly used words less important and increases the importance of words that are not used very much in a collection of texts. In this case, the entire collection of text is the series, while each document or body of text is each individual season.\r\nWe can take both of these, the term frequency (tf) and the inverse document frequency (idf), and multiply them together to create the tf-idf statistic. Basically, the tf-idf statistic will identify words that are important in each individual season within the entire series. I hope that makes sense.\r\nEven though I am not an expert on tf-idf, I needed to explain it because that is the x-axis of the next several graphics.\r\nHere we go:\r\n\r\nDownsizing is the talk of the town in season 1. Be sure to form an alliance with your co-workers if you don’t want to be next.\r\n\r\nAlso, there are a couple of odd words here. And there are a few odd words in pretty much every season. Where do they come from?\r\nHere are a couple lines taken directly from the transcript data:\r\nOh! Welcome to my convenience store. Would you like some googi googi? I have some very delicious googi, googi, only 99 cents plus tax. Try my googi, googi. Try my googi, googi. Try my googi, googi. Try my… - Michael\r\nWatch this. This is Moe. Nyuck-nyuck-nyuck-nyuck-nyuck. Mee! Ah, right here. Three Stooges. Oh, Pam. It’s a guy thing, Pam. I’m sort of a student of comedy. Watch this. Here we go. I’m Hitler. Adolf Hitler. - Michael\r\n\r\nSome favorites:\r\nAnd, I feel God in this Chili’s tonight. WHOOOOOOOO!!!! - Pam\r\nWell, surely this uh, review is a formality because of what happened uh, at our meeting in the parking lot of Chili’s. - Michael\r\nDon’t think of it that way. It’s like, urine goes all over the place. You know, there’s no controlling it. It just… goes - Michael\r\n\r\nMakes sense:\r\nThere are five stages to grief, which are denial, anger, bargaining, depression, and acceptance. And right now, out there, they’re all denying the fact that they’re sad. And that’s hard. And it’s making them all angry. And it is my job to try to get them all the way through to acceptance. And if not acceptance, then just depression. If I can get them depressed, then I’ll have done my job. - Michael\r\n\r\nRabies is serious.\r\nMichael Scott’s Dunder-Mifflin, Scranton, Meredith Palmer memorial, celebrity rabies awareness, fun run race for the cure, this is Pam. - Pam\r\nAnd also there is no such thing as a rabies doctor. - Pam\r\nInspirational:\r\nFinishing that 5k, was the hardest thing I have ever had to do. I ate more fettuccine alfredo and drank less water, than I have in my entire life. People always talk about triumphs of the human spirit, well today I had a triumph of the human body. That’s why everybody was applauding for me at the end. My guts and my heart, and while I eventually puked my guts out, I never puked my heart out. And I’m very, very proud of that. - Michael\r\n\r\nSo I love Willy Wonka. That golden ticket scene is so inspiring to me that that’s where I cam up with that idea. - Dwight\r\nNow it’s kind of interesting that, from Season 6 and on, a lot of the words that are deemed important are new characters that are introduced. This makes me think that they kept adding new characters to try and keep things interesting. Not say that that is what the data is saying per se, but it’s a thought.\r\n\r\nRecyclops destroys! - Dwight\r\n\r\nCroak, croak, croak. Bullfrog in love. - Darryl, Kevin, Andy\r\n\r\n\r\nIt’s a Hazmat suit. That stands for hazaderous materials men’s suit wearing. If you rent more than four times a year, it just makes sense to buy. Is there anyone else here that is lice free? Excellent. Do you have your own hazmat suits?\r\nConclusion\r\nThat’s all for now! There is so much more that we could do here. In the future, I want to try and do some more advanced things, like a machine learning model or two. If you have specific questions that you think can be answered from the data, put them in the comments below. Thanks so much for reading!\r\n\r\n\r\n\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-20T23:37:48-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-26-pdf-scraping-data-wrangling/",
    "title": "PDF scraping can be easy, but data wrangling is often not!",
    "description": "An inside look at some R code in the wild. This post is a clarification of a previous post: PDF scraping with R is easy! I changed easy to possible. While scraping data from PDFs can often be accomplished with just a few tools, wrangling the scraped data can often take time.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-07-26",
    "categories": [],
    "contents": "\r\n\r\nMotivation\r\nThis blog is still very new. My previous post, PFD scraping with R is easy!, was one of the most popular posts that I have written thus far. That being said, it gathered some criticism on Reddit and elsewhere. A few R users, possibly with more experience than myself, argued that PDFs in the wild can be more difficult to scrape. This is fair. It’s not always easy. While it is true that some PDFs are harder to scrape than others, I affirm that most cases can be solved with just a few number of tools.\r\nOne person who read that initial post works at a company where they often receive data from vendors in PDF form. Not sure why anyone would send data in a PDF, but that’s the world we live in. Thankfully, the vendors’ PDFs, though they have different data, share a similar format and structure to them. There is a pattern. The reader, who for anonymity’s sake we will refer to henceforth as James, asked for some help in programmatically gathering the data. So, yesterday I spent some time helping him. In total, reading the data into R took about 2 seconds, while tidying the data took about 2 hours. Later on I will showcase an edited version (for data privacy reasons) of the script that I wrote.\r\nTo clarify, the difficult part of PDF scraping is often not the PDF scraping itself, but rather the data wrangling that can accompany it.\r\nTools\r\nAs I mentioned in the first post, R packages like pdftools and tabulizer provide fantastic tools for gathering data from PDFs. Many PDFs can be scraped with just one line of code:\r\n\r\n\r\ndat <- pdftools::pdf_text(\"path/to/data.pdf\")\r\n\r\n\r\n\r\nBut, oh no, sometimes PDFs can come from scans or images! What do I do now?\r\npdftools::pdf_ocr_text can often help with those cases, using optical character recognition:\r\n\r\n\r\ndat <- pdftools::pdf_ocr_text(\"path/to/data.pdf\")\r\n\r\n\r\n\r\nAlso, as mentioned previously, the tabulizer R package is excellent for extracting tables from PDFs. With tabulizer::extract_tables, you can often dramatically cut down on the data wrangling that you would have to do otherwise. tabulizer is awesome, but relies on Java, so installing it can be a bit more complicated than usual.\r\nTaken from tabulizer’s GitHub:\r\n\r\n\r\nlibrary(\"tabulizer\")\r\nf <- system.file(\"examples\", \"data.pdf\", package = \"tabulizer\")\r\nout1 <- extract_tables(f)\r\nstr(out1)\r\n## List of 4\r\n##  $ : chr [1:32, 1:10] \"mpg\" \"21.0\" \"21.0\" \"22.8\" ...\r\n##  $ : chr [1:7, 1:5] \"Sepal.Length \" \"5.1 \" \"4.9 \" \"4.7 \" ...\r\n##  $ : chr [1:7, 1:6] \"\" \"145 \" \"146 \" \"147 \" ...\r\n##  $ : chr [1:15, 1] \"supp\" \"VC\" \"VC\" \"VC\" ...\r\n\r\n\r\n\r\nNow, I understand that it is possible that all of these tools still may fail. That hasn’t happened for me yet, but it’s certainly possible. If you know of other tools, especially ones that could handle more extreme use cases, please comment below and share them with everyone. For example, one Reddit user mentioned the readtext package, which I think is definitely worth looking into.\r\nWild PDFs, wild code\r\nAs mentioned previously, here is an edited version of the code I wrote for James. Unfortunately, for data privacy reasons I cannot show you the actual PDFs. Suffice it to say that they were annoyingly organized and not ideal for any real data analysis. The data was more free form than tablular, so pdftools was my weapon of choice.\r\nInstead of walking you through each line of code, line by line, I will speak more generally. I want to give you just an example of code in the wild. In this script, I used quite a bit of purrr and some helper functions, which you can learn more about here. As always, for a more in-depth look at the pipe operator %>% and the many tidyverse functions used here, I would refer you to R for Data Science.\r\nHere we go.\r\n\r\n\r\n# packages \r\n\r\n# I use pacman for package management - https://github.com/trinker/pacman\r\n\r\npacman::p_load(tidyverse, # all the things\r\n               pdftools,  # reading in pdfs\r\n               fs)        # working with files \r\n               \r\npacman::p_load_gh('averyrobbins1/sometools') # my personal R package\r\n\r\n# read data here\r\n\r\ndat_all <- fs::dir_ls(\"data\") %>% \r\n    map(pdf_text) %>% \r\n    map(c(3)) %>% \r\n    map(split_data) \r\n\r\n\r\n\r\nHaving stored all of the PDFs received from James in a single directory (data), fs::dir_ls allows us to list all of those files. Then, using purrr::map, we read in each PDF, grab just the third page, and split the data according to our needs with a helper function defined below.\r\n\r\n\r\n# in my actual script, split_data was obviously defined before using it above\r\n\r\nsplit_data <- function(data) {\r\n    data %>%\r\n        str_split(\"(\\\\-{15,})\") %>% \r\n        map(~ str_split(.x, \"\\\\n\"))\r\n}\r\n\r\n\r\n\r\nAgain, we cannot go into too much detail because of privacy reasons, but I hope you will be able to recognize the importance of data wrangling in your own programming toolbelt.\r\nRemember, when reading in free form PDF data using pdftools, it will sometimes show up as a giant amorphous text blob. This is simply the nature of untamed PDFs. The helper function below is used to wrangle and tidy most of the data that we desire. I won’t really go into it, but notice the use of many tidyverse packages such as stringr, tidyr, and dplyr. Also, most text cleaning will involve a decent amount of regular expressions, or regex. If you are unfamiliar with regex, it is the gibberish inside all of the str_* functions, which allows us to manipulate text data. When working with stringr and regex, I pretty much always have the stringr cheat sheet in front of me.\r\n\r\n\r\nget_footers <- function(footers) {\r\n    footers %>%\r\n        str_extract(\"^[:digit:]{8}.+\") %>%\r\n        .[!is.na(.)] %>% \r\n        str_remove_all(\"[:digit:]{9,}\") %>%\r\n        enframe(\"row_num\", \"product_info\") %>% \r\n        mutate(product_info = str_trim(product_info)) %>% \r\n        filter(product_info != \"abc_company\") %>% \r\n        mutate(product_info = str_remove_all(product_info, \"[:digit:]{8}\"),\r\n               product_info = str_trim(product_info),\r\n               product_name =\r\n                   str_extract(product_info, \"(?<=[:digit:])\\\\s.+\") %>%\r\n                   str_extract(\"(?<=[:digit:])\\\\s.+\") %>%\r\n                   str_extract(\"(?<=[:digit:])\\\\s.+\") %>%\r\n                   str_extract(\"\\\\s.+(?=\\\\s[:digit:])\") %>%\r\n                   str_extract(\"\\\\s.+(?=\\\\s[:digit:])\") %>%\r\n                   str_extract(\"\\\\s.+(?=\\\\s[:digit:])\") %>%\r\n                   str_extract(\"\\\\s.+(?=\\\\s[:digit:])\") %>%\r\n                   str_remove(\"\\\\d.+\") %>%\r\n                   str_trim(side = \"right\"),\r\n               product_info = str_squish(product_info),\r\n               product_info = str_remove_all(product_info, product_name)\r\n        ) %>% \r\n        separate(col = product_info,\r\n                 into = c(\"product_code\", \"qty1\", \"qty2\",\r\n                          \"size\", \"case\", \"pack\", \"cost\",\r\n                          \"extended_cost\"),\r\n                 sep = \" \") %>% \r\n        mutate(product_name = str_trim(product_name)) %>% \r\n        select(-row_num) %>% \r\n        relocate(product_name, .after = \"qty1\")\r\n}\r\n\r\n\r\n\r\nWe map our get_footers helper function to each section of every PDF, and then tidy up the results with enframe and unnest.\r\n\r\n\r\ndat_footers <- dat_all %>% \r\n    map(~ .x[[1]][[2]]) %>% \r\n    map(get_footers)\r\n\r\ndat_footers_tidy <- dat_footers %>% \r\n    enframe(\"name\", \"info\") %>% \r\n    unnest(info)\r\n\r\n\r\n\r\nSo far we tidied up the bulk of the desired data. We went from one messy character vector to a tidy dataframe with eight columns.\r\nNow we just need a few more items from our PDF, namely an id number, a date, and another id number.\r\nTo get the first number, we need more purrr, more stringr, and more helper functions.\r\n\r\n\r\ndat_headers <- dat_all %>% \r\n    map(~ .x[[1]][[1]]) %>% \r\n    map(str_squish)\r\n\r\nget_id1 <- function(data) {\r\n    data %>% \r\n        str_squish() %>% \r\n        .[2] %>% \r\n        str_extract(\"P\\\\.O\\\\.#\\\\s\\\\d{7}\") %>% \r\n        str_remove(\"P\\\\.O\\\\.#\\\\s\") %>% \r\n        as.integer()\r\n}\r\n\r\ndat_id1 <- dat_headers %>% \r\n    map(get_id1) %>% \r\n    enframe(\"name\", \"id1\") %>% \r\n    mutate(id1 = simplify(id1))\r\n\r\n\r\n\r\nMore of the same for the date.\r\n\r\n\r\ndat_date <- dat_headers %>% \r\n    map(~ str_extract(.x, \"ESTIMATED PICK UP DATE\\\\:.+\")) %>% \r\n    map(~ .x[!is.na(.x)]) %>% \r\n    enframe(\"name\", \"estimated_date\") %>% \r\n    mutate(estimated_date = simplify(estimated_date),\r\n           estimated_date = str_remove(estimated_date,\r\n                                       \"ESTIMATED PICK UP DATE\\\\:\\\\s\"),\r\n           estimated_date =\r\n               parse_date(estimated_date, format = \"%m/%d/%Y\"))\r\n\r\n\r\n\r\nAnd for the last id number.\r\n\r\n\r\ndat_id2 <- dat_headers %>% \r\n    map(~ str_extract(.x, \"DC \\\\#\\\\:\\\\s\\\\d+\")) %>% \r\n    map(~ .x[!is.na(.x)]) %>% \r\n    enframe(\"name\", \"id2\") %>% \r\n    mutate(id2 = simplify(id2),\r\n           id2 = parse_number(id2))\r\n\r\n\r\n\r\nLastly, we need to join all of our data together into a single dataframe.\r\n\r\n\r\ndat_master <- dat_footers_tidy %>% \r\n    left_join(dat_id1) %>% \r\n    left_join(dat_date) %>% \r\n    left_join(dat_id2)\r\n\r\n\r\n\r\nThe now tidy dataframe of 11 columns can be easily exported as a csv and used in Excel or wherever.\r\n\r\n\r\nreadr::write_csv(dat_master, \"path/to/data.csv\")\r\n\r\n\r\n\r\nThe big picture\r\nTake a second to think about what we have done.\r\nWe have turned difficult to use data, stored in individual PDFs, into one easily analyzable csv that can be used further in R or in virtually any other analytical framework.\r\nWe have automated a previously manual process. The code should be robust and flexible enough to be able to handle any future PDFs of this structure and format with little to no tweaking. By simply running a single script, we can save potentially hundreds of hours of manual, error prone data entry. This frees up employees to spend their time on more important things, like actually gaining insight from the data.\r\nThank you for reading! Happy coding!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-20T23:26:47-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-22-pdf-scraping/",
    "title": "PDF scraping with R is possible!",
    "description": "You can definitely scrape data from PDFs; it really is not that difficult. The data wrangling is usually the hard part.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-07-22",
    "categories": [],
    "contents": "\r\nTwo anecdotes\r\nIf something can be automated, then it probably should be. In this post our focus will be on scraping data from PDFs.\r\nPoor unfortunate souls\r\nWhat I am about to tell you is true.\r\nA year or two ago I went with one of my professors and some fellow students to visit a few different companies in another state. While visiting one of the companies, we happened to stumble upon two interns that shared a cubicle together. They were working on the same project, and it was estimated that this specific project would last them their entire internship. The bulk of the project involved a seemingly tedious task: collecting data from PDFs and inputing it into excel. The PDFs appeared to have very similar, if not the same, structures and formats to them. They were doing this manually, and it was supposed to last them two to three months.\r\n\r\nFrankly, we were pained to hear what they were doing. Even if learning how to programmatically scrape data from a large amount of PDFs took weeks or a month to learn, that is far better than tediously entering numbers into a spreadsheet for days on end.\r\nGuess what. Things can be automated and made much easier with just a little programming. Google something. Ask a friend. Read some blog posts or a book. Anything. This example with the interns is just one of many examples of how having some basic programming skills can make all the difference in the world.\r\nNot as painful, mostly for fun\r\nMy wife recently found herself in a rather annoying situation. After graduating from college, her Microsoft Office subscription that she got through the school had changed. For some reason, she could only view files on her Mac, but not edit them. She couldn’t even copy and paste text from a Word document. It was weird.\r\nShe was trying to update her resume, but she was locked out of the file. Our overkill solution was to take a screen shot of the Word document, convert it to a PDF, email me the PDF, and have me use R to scrape the data from it. There surely exist simpler solutions, but I, perhaps selfishly, wanted to help by using R. I just had to remember how to scrape data from PDFs. Turns out it is super simple.\r\nPDF scraping\r\nInstall the pdftools package for reading data from a PDF, and optionally the readr package for reading and writing data to and from R.\r\n\r\n\r\n# install.packages(c(\"pdftools\", \"readr\"))\r\n\r\nlibrary(pdftools)\r\nlibrary(readr)\r\n\r\n\r\n\r\nFor this example, I am using an edited version of my own resume that I have on GitHub. Using the url and some base R functions, I download the resume from GitHub and store it in a temporary file. After that I just use the pdf_text function from pdftools to grab all of the text data from my resume. It reads all of the data into a single character vector, so often times, and depending on the task at hand, you would need to do a bit of data wrangling to get your data in the desired format. The pdf-scraping, however, is incredibly simple.\r\n\r\n\r\nurl <- \"https://github.com/averyrobbins1/robbins-data/blob/master/documents/avery-robbins-resume.pdf?raw=true\"\r\n\r\ntemp <- tempfile()\r\n\r\ndownload.file(url, temp, mode = \"wb\")\r\n\r\ndat <- pdftools::pdf_text(temp)\r\n\r\n\r\n\r\nIf the PDF comes from a scan, you will probably have to use pdf_ocr_text instead. See ?pdftools::pdf_ocr_text\r\nFor our purposes, I just wrote the data back out to a csv, and my wife just copied and pasted the text and put it in a google doc. Alternatively, you could also write the data to a Word doc if you need to.\r\n\r\n\r\nreadr::write_file(dat, \"resume-robbins-avery.doc\")\r\n\r\n\r\n\r\n\r\nLastly.\r\nIf those poor interns had all of the PDFs in a single directory, they probably could have written similar code to the following:\r\n\r\n\r\ndat <- fs::dir_ls(\"some_directory\") %>% \r\n    purrr::walk(~ pdftools::pdf_text(.x))\r\n\r\n\r\n\r\nList all of the PDF files in some directory containing just those files. Then walk through the directory and run pdf_text on each PDF. walk is similar to purrr::map. ?walk\r\nAfter having scraped all of the data, the interns could have then probably worked on wrangling the data for a week or two.\r\nConclusion\r\nThat’s pretty much it. Check out the pdftools GitHub to learn more about the package. tabulizer is another good package for extracting tables from PDFs.\r\nThank you for reading! Take care.\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-20T23:20:01-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-07-13-webscraping-atla/",
    "title": "Databending - web scraping Avatar: The Last Airbender",
    "description": "You get better at webscraping with R by webscraping with R. This is my attempt to webscrape a bunch of data on the epic show, Avatar: The Last Airbender.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-07-13",
    "categories": [],
    "contents": "\r\n\r\nMotivation\r\nAvatar: The Last Airbender is arguably the greatest animated TV show of all time. Being such a great show, it deserves to have great, well organized data. This post details my web scraping of the Avatar Wiki and IMDB for transcript and ratings data. I was inspired to collect Avatar: The Last Airbender data after messing around with the schrute R package that contains transcript data from The Office.\r\n\r\n\r\n# install.packages(\"schrute\")\r\ndplyr::glimpse(schrute::theoffice)\r\n\r\n\r\nRows: 55,130\r\nColumns: 12\r\n$ index            <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 1...\r\n$ season           <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r\n$ episode          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1...\r\n$ episode_name     <chr> \"Pilot\", \"Pilot\", \"Pilot\", \"Pilot\", \"Pil...\r\n$ director         <chr> \"Ken Kwapis\", \"Ken Kwapis\", \"Ken Kwapis\"...\r\n$ writer           <chr> \"Ricky Gervais;Stephen Merchant;Greg Dan...\r\n$ character        <chr> \"Michael\", \"Jim\", \"Michael\", \"Jim\", \"Mic...\r\n$ text             <chr> \"All right Jim. Your quarterlies look ve...\r\n$ text_w_direction <chr> \"All right Jim. Your quarterlies look ve...\r\n$ imdb_rating      <dbl> 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, 7.6, ...\r\n$ total_votes      <int> 3706, 3706, 3706, 3706, 3706, 3706, 3706...\r\n$ air_date         <fct> 2005-03-24, 2005-03-24, 2005-03-24, 2005...\r\n\r\nHere we go!\r\nAs usual, here are the required packages for collecting and manipulating the data:\r\n\r\n\r\n# packages\r\n\r\n# install.packages(c(\"tidyverse\", \"rvest\", \"tictoc\"))\r\n\r\nlibrary(tidyverse) # all the things\r\nlibrary(rvest) # web scraping\r\nlibrary(glue) # easily interpret R code inside of strings\r\nlibrary(tictoc) # time code and stuff\r\n\r\n\r\n\r\nWeb scraping tools\r\nHonestly, I don’t have too much experience web scraping, but the best way to get better at something is to do that thing more. To make web scraping easier there are just a couple of tools that I use.\r\nrvest - Hadley Wickham’s R package for web scraping\r\nThere are many tutorials and blog posts teaching how to use rvest to web scrape data. A quick google search “web scraping with r rvest” will bring up a lot of good stuff.\r\nSelectorGadget - a CSS selector generation chrome extension\r\nAfter installing and activating the chrome extension, click on a page element that you are interested in, and then SelectorGadget will “generate a minimal CSS selector for that element.” Below is an image of what that would look like. Look up the extension to learn more.\r\n\r\nYou can then take what SelectorGadget gives you, and put that into rvest::html_nodes as shown below.\r\nArmed with these tools, we can dive into the mess of it all.\r\nWeb scraping\r\nTranscripts\r\nThe first page of interest on the Avatar Wiki has a list of every episode in the series with links to their individual transcripts.\r\n\r\nUltimately, we want to scrape all of the transcript text from pages like this:\r\n\r\nTaking a look at just a few of the transcript pages, we can see a handy pattern in the url for each page.\r\n\r\n\r\n\r\nBasically each one starts with https://avatar.fandom.com/wiki/Transcript: and is followed simply by the episode name, like so: https://avatar.fandom.com/wiki/Transcript:The_Boy_in_the_Iceberg.\r\nArmed with that knowledge, we can use some nifty R magic to\r\nscrape the names of the episodes (chapters) from the main transcript page, and\r\nuse the name of the chapters to iterate through and scrape each individual chapter transcript\r\nI hope that makes sense.\r\nFirst, we’ll read in the transcript page with rvest::read_html, input the CSS selector that we got using SelectorGadget into rvest::html_nodes, and extract out all of the text with rvest::html_text.\r\n\r\n\r\n# main transcript page url\r\n\r\navatar_wiki <- read_html(\"https://avatar.fandom.com/wiki/Avatar_Wiki:Transcripts\")\r\n\r\n# scrape the chapter names\r\n\r\nchapters <- avatar_wiki %>% \r\n  html_nodes(\"td\") %>% \r\n  html_text()\r\n\r\n\r\n\r\nNow we have to beautify the ugly mess of text that comes in.\r\ntibble::enframe quite easily turns our blob of text into a nice tibble. There is still more cleaning required, but getting your data into a tibble often makes your life easier. All of your favorite dplyr functions like filter and mutate work nicely with tibbles.\r\nWe scraped more data than we need, so we will do some filtering. We only want the original Avatar data, not Legend of Korra (although maybe I should scrape that later), so we’ll just get the first 70 rows. There are some rows that contain more data than just single chapters (len < 50), so we’ll get rid of those as well.\r\nLastly, we’ll clean up the chapter column with some stringr functions and filter out a couple of bonus episodes that we don’t really want.\r\n\r\n\r\n# to see rows with too much text that are actually many chapters\r\n\r\nchapters %>% \r\n  enframe(name = \"row_num\", value = \"chapter\") %>% \r\n  filter(row_num %in% 1:70) %>%\r\n  mutate(len = str_length(chapter))\r\n\r\n\r\n# A tibble: 70 x 3\r\n   row_num chapter                                                 len\r\n     <int> <chr>                                                 <int>\r\n 1       1 \"\\n0\\n\\n\\\"Unaired pilot\\\"\\nBook One: Water\\n\\n1\\n\\n\\~  5743\r\n 2       2 \"\\\"Unaired pilot\\\"\\n\"                                    16\r\n 3       3 \"\\n1\\n\\n\\\"The Boy in the Iceberg\\\"\\n2\\n\\n\\\"The Avata~   278\r\n 4       4 \"\\\"The Boy in the Iceberg\\\"\\n\"                           25\r\n 5       5 \"\\\"The Avatar Returns\\\"\\n\"                               21\r\n 6       6 \"\\\"The Southern Air Temple\\\"\\n\"                          26\r\n 7       7 \"\\\"The Warriors of Kyoshi\\\"\\n\"                           25\r\n 8       8 \"\\\"The King of Omashu\\\"\\n\"                               21\r\n 9       9 \"\\\"Imprisoned\\\"\\n\"                                       13\r\n10      10 \"\\\"Winter Solstice, Part 1: The Spirit World\\\"\\n\"        44\r\n# ... with 60 more rows\r\n\r\nchapters <- chapters %>% \r\n  enframe(name = \"row_num\", value = \"chapter\") %>% \r\n  filter(row_num %in% 1:70) %>% \r\n  mutate(len = str_length(chapter)) %>% \r\n  filter(len < 50) %>% \r\n  mutate(chapter = str_remove_all(chapter, pattern = \"\\\"\"),\r\n         chapter = str_trim(chapter, side = \"both\"),\r\n         chapter = str_remove_all(chapter, \" \\\\(commentary\\\\)\")) %>% \r\n  filter( !(chapter %in% c(\"Unaired pilot\", \"Escape from the Spirit World\")) )\r\n\r\nchapters\r\n\r\n\r\n# A tibble: 61 x 3\r\n   row_num chapter                                     len\r\n     <int> <chr>                                     <int>\r\n 1       4 The Boy in the Iceberg                       25\r\n 2       5 The Avatar Returns                           21\r\n 3       6 The Southern Air Temple                      26\r\n 4       7 The Warriors of Kyoshi                       25\r\n 5       8 The King of Omashu                           21\r\n 6       9 Imprisoned                                   13\r\n 7      10 Winter Solstice, Part 1: The Spirit World    44\r\n 8      11 Winter Solstice, Part 2: Avatar Roku         39\r\n 9      12 The Waterbending Scroll                      26\r\n10      13 Jet                                           6\r\n# ... with 51 more rows\r\n\r\nNice.\r\nNow that we have all the chapter names, we can use them to scrape each chapter’s transcript page. My general philosophy for automation and iteration is to do something once or twice, then extract the pattern and do that same thing for everything.\r\nRemember that each chapter transcript page pretty much looks like this:\r\n\r\nOkay, looking at the code below there are a couple of things to talk about. Notice the “table.wikitable” in html_nodes below. Unfortunately, sometimes you cannot get super helpful results with SelectorGadget. You can, however, inspect the page (right click and go to Inspect) and grab specific html elements. That is generally what I do if SelectorGadget is giving me any trouble. We are also using html_table instead of html_text.\r\nThe table that is returned is a list with two elements: one with character names, and the other with spoken words and scene directions. So, we’ll use purrr::pluck, purrr::simplify, and enframe on both elements, respectively, and give each tibble a new column with the chapter link to serve as an id column.\r\nFirst, The Boy in the Iceberg.\r\n\r\n\r\niceberg <- read_html(\"https://avatar.fandom.com/wiki/Transcript:The_Boy_in_the_Iceberg\")\r\n\r\ncharacters <- iceberg %>% \r\n  html_nodes(\"table.wikitable\") %>% \r\n  html_table() %>% \r\n  pluck(1) %>% \r\n  simplify() %>% \r\n  enframe(value = \"value1\") %>% \r\n  mutate(chapter1 = \"https://avatar.fandom.com/wiki/Transcript:The_Boy_in_the_Iceberg\") %>% \r\n  select(-name)\r\n\r\ntext <- iceberg %>% \r\n  html_nodes(\"table.wikitable\") %>% \r\n  html_table() %>% \r\n  pluck(2) %>% \r\n  simplify() %>% \r\n  enframe(value = \"value2\") %>% \r\n  mutate(chapter2 = \"https://avatar.fandom.com/wiki/Transcript:The_Boy_in_the_Iceberg\") %>% \r\n  select(-name)\r\n\r\niceberg2 <- bind_cols(characters, text) %>% \r\n  rename(character = value1, text = value2)\r\n\r\niceberg2\r\n\r\n\r\n# A tibble: 223 x 4\r\n   character chapter1            text               chapter2          \r\n   <chr>     <chr>               <chr>              <chr>             \r\n 1 \"Katara\"  https://avatar.fan~ \"Water. Earth. Fi~ https://avatar.fa~\r\n 2 \"\"        https://avatar.fan~ \"As the title car~ https://avatar.fa~\r\n 3 \"Sokka\"   https://avatar.fan~ \"It's not getting~ https://avatar.fa~\r\n 4 \"\"        https://avatar.fan~ \"The shot pans qu~ https://avatar.fa~\r\n 5 \"Katara\"  https://avatar.fan~ \"[Happily surpris~ https://avatar.fa~\r\n 6 \"Sokka\"   https://avatar.fan~ \"[Close-up of Sok~ https://avatar.fa~\r\n 7 \"\"        https://avatar.fan~ \"Behind Sokka, Ka~ https://avatar.fa~\r\n 8 \"Katara\"  https://avatar.fan~ \"[Struggling with~ https://avatar.fa~\r\n 9 \"\"        https://avatar.fan~ \"The bubble conta~ https://avatar.fa~\r\n10 \"Katara\"  https://avatar.fan~ \"[Exclaims indign~ https://avatar.fa~\r\n# ... with 213 more rows\r\n\r\nSecond, The Avatar Returns.\r\n\r\n\r\nreturns <- read_html(\"https://avatar.fandom.com/wiki/Transcript:The_Avatar_Returns\")\r\n\r\ncharacters2 <- returns %>% \r\n  html_nodes(\"table.wikitable\") %>% \r\n  html_table() %>% \r\n  pluck(1) %>% \r\n  simplify() %>% \r\n  enframe(value = \"value1\") %>% \r\n  mutate(chapter1 = \"https://avatar.fandom.com/wiki/Transcript:The_Avatar_Returns\") %>% \r\n  select(-name)\r\n\r\ntext2 <- returns %>% \r\n  html_nodes(\"table.wikitable\") %>% \r\n  html_table() %>% \r\n  pluck(2) %>% \r\n  simplify() %>% \r\n  enframe(value = \"value2\") %>% \r\n  mutate(chapter2 = \"https://avatar.fandom.com/wiki/Transcript:The_Avatar_Returns\") %>% \r\n  select(-name)\r\n\r\nreturns2 <- bind_cols(characters2, text2) %>% \r\n  rename(character = value1, text = value2)\r\n\r\nreturns2\r\n\r\n\r\n# A tibble: 182 x 4\r\n   character  chapter1            text              chapter2          \r\n   <chr>      <chr>               <chr>             <chr>             \r\n 1 \"Katara\"   https://avatar.fan~ Water. Earth. Fi~ https://avatar.fa~\r\n 2 \"\"         https://avatar.fan~ The episode open~ https://avatar.fa~\r\n 3 \"Village ~ https://avatar.fan~ [Joyfully.] Yay!~ https://avatar.fa~\r\n 4 \"\"         https://avatar.fan~ Some of them run~ https://avatar.fa~\r\n 5 \"Sokka\"    https://avatar.fan~ [Angrily.] I kne~ https://avatar.fa~\r\n 6 \"Katara\"   https://avatar.fan~ [Protesting.] Aa~ https://avatar.fa~\r\n 7 \"Aang\"     https://avatar.fan~ [Sheepishly, as ~ https://avatar.fa~\r\n 8 \"Kanna\"    https://avatar.fan~ [Worriedly.] Kat~ https://avatar.fa~\r\n 9 \"Aang\"     https://avatar.fan~ [Sorrowfully.] D~ https://avatar.fa~\r\n10 \"Sokka\"    https://avatar.fan~ [Angry and trium~ https://avatar.fa~\r\n# ... with 172 more rows\r\n\r\nNow that we successfully executed the code on two different chapters, we will use that same pattern for the rest. Well, almost the same. For some reason (for the next 59 chapters) we have to pluck “X1” and “X2” instead of the elements 1 and 2.\r\nBefore we get to that, we’ll use the chapter column from chapters and manipulate the data so that we get a character vector of the links for each chapter transcript page. Notice how we have to replace the ' with %27 because of how the links are set up.\r\nWe’ll use glue::glue for easy string interpolation. glue is vectorized: no need to write a for loop yourself.\r\n\r\n\r\nchapter_urls <- chapters %>% \r\n  filter( !(chapter %in% c(\"The Boy in the Iceberg\", \"The Avatar Returns\")) ) %>% \r\n  mutate(chapter = str_replace_all(chapter, pattern = \" \", replacement = \"_\"),\r\n         chapter = str_replace_all(chapter, pattern = \"\\'\", replacement = \"%27\")) %>% \r\n  pull(chapter)\r\n\r\nfull_urls <- glue(\"https://avatar.fandom.com/wiki/Transcript:{chapter_urls}\")\r\n\r\nfull_urls %>% head(5)\r\n\r\n\r\nhttps://avatar.fandom.com/wiki/Transcript:The_Southern_Air_Temple\r\nhttps://avatar.fandom.com/wiki/Transcript:The_Warriors_of_Kyoshi\r\nhttps://avatar.fandom.com/wiki/Transcript:The_King_of_Omashu\r\nhttps://avatar.fandom.com/wiki/Transcript:Imprisoned\r\nhttps://avatar.fandom.com/wiki/Transcript:Winter_Solstice,_Part_1:_The_Spirit_World\r\n\r\nNow use purrr::map to apply our code from before to each url (.x) in full_urls. Sometimes I like to use the tictoc package to time code that takes a little longer. tic starts the timer, while toc ends it.\r\n\r\n\r\ntic()\r\ncharacters_all <- full_urls %>% \r\n  map(~ read_html(.x) %>%\r\n            html_nodes(\"table.wikitable\") %>%\r\n            html_table() %>% \r\n            pluck(\"X1\") %>% \r\n            simplify() %>% \r\n            enframe() %>% \r\n            mutate(chapter = .x))\r\ntoc()\r\n\r\n\r\n13.62 sec elapsed\r\n\r\ntic()\r\ntranscripts_all <- full_urls %>% \r\n  map(~ read_html(.x) %>%\r\n        html_nodes(\"table.wikitable\") %>%\r\n        html_table() %>% \r\n        pluck(\"X2\") %>% \r\n        simplify() %>% \r\n        enframe() %>% \r\n        mutate(chapter = .x))\r\ntoc()\r\n\r\n\r\n13.91 sec elapsed\r\n\r\nAll of them worked great, except for something weird that happened with The Tales of Ba Sing Se chapter. characters_all has one too many rows for some reason, and in transcripts_all the value column created from using enframe is a list-column instead of a character vector like all of the others.\r\n\r\n\r\ncharacters_all[[33]]\r\n\r\n\r\n# A tibble: 178 x 3\r\n    name value    chapter                                             \r\n   <int> <chr>    <glue>                                              \r\n 1     1 \"\"       https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n 2     2 \"Katara\" https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n 3     3 \"\"       https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n 4     4 \"Toph\"   https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n 5     5 \"Katara\" https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n 6     6 \"Toph\"   https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n 7     7 \"Katara\" https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n 8     8 \"Toph\"   https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n 9     9 \"Katara\" https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n10    10 \"\"       https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n# ... with 168 more rows\r\n\r\ntranscripts_all[[33]]\r\n\r\n\r\n# A tibble: 6 x 3\r\n   name value     chapter                                             \r\n  <int> <list>    <glue>                                              \r\n1     1 <chr [37~ https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n2     2 <chr [31~ https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n3     3 <chr [31~ https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n4     4 <chr [25~ https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n5     5 <chr [53~ https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n6     6 <NULL>    https://avatar.fandom.com/wiki/Transcript:The_Tales~\r\n\r\nThat just means we need to do some additional data wrangling. We’ll use purrr::modify_at to modify the 33rd element of each list\r\n\r\n\r\ncharacters_all2 <- characters_all %>% \r\n  modify_at(33, ~ filter(.x, row_number() != 178)) %>% \r\n  bind_rows()\r\n\r\ntranscripts_all2 <- transcripts_all %>% \r\n  modify_at(33, ~ unnest(.x, cols = value)) %>% \r\n  bind_rows()\r\n\r\n\r\n\r\nEverything matches up now, so we’ll bind the character and transcript data together.\r\n\r\n\r\nfull_transcript <- bind_cols(\r\n  characters_all2 %>% select(character = value, chapter1 = chapter),\r\n  transcripts_all2 %>% select(text = value, chapter2 = chapter)\r\n  )\r\n\r\nfull_transcript\r\n\r\n\r\n# A tibble: 12,984 x 4\r\n   character chapter1             text             chapter2           \r\n   <chr>     <glue>               <chr>            <glue>             \r\n 1 \"\"        https://avatar.fand~ The episode beg~ https://avatar.fan~\r\n 2 \"Aang\"    https://avatar.fand~ [Excitedly.] Wa~ https://avatar.fan~\r\n 3 \"Katara\"  https://avatar.fand~ [Cautiously.] A~ https://avatar.fan~\r\n 4 \"Aang\"    https://avatar.fand~ [Smiling broadl~ https://avatar.fan~\r\n 5 \"Katara\"  https://avatar.fand~ [Cautiously.] I~ https://avatar.fan~\r\n 6 \"Aang\"    https://avatar.fand~ [Happily.] I kn~ https://avatar.fan~\r\n 7 \"\"        https://avatar.fand~ Aang jumps off ~ https://avatar.fan~\r\n 8 \"Aang\"    https://avatar.fand~ [Cheerfully.] W~ https://avatar.fan~\r\n 9 \"Sokka\"   https://avatar.fand~ [Grunting sleep~ https://avatar.fan~\r\n10 \"\"        https://avatar.fand~ Sokka turns aro~ https://avatar.fan~\r\n# ... with 12,974 more rows\r\n\r\nDon’t forget about the first two episodes. We’ll bind those to the data as well.\r\n\r\n\r\ndat <- bind_rows(iceberg2, returns2, full_transcript)\r\n\r\n\r\n\r\nNow onto more data wrangling:\r\nClean up the chapter column\r\nCreate a book column\r\nCreate book_num and chapter_num columns\r\nCreate an id column\r\nMutate the character column to add the “Scene Description” value\r\n\r\n\r\ndat <- dat %>% \r\n  select(character, text, chapter = chapter1) %>% \r\n  mutate(\r\n    chapter = str_remove_all(chapter, \"https://avatar.fandom.com/wiki/Transcript:\"),\r\n    chapter = str_replace_all(chapter, \"_\", \" \"),\r\n    chapter = str_replace_all(chapter, pattern = \"%27\", replacement = \"\\'\")) %>% \r\n  left_join(\r\n    chapters %>% \r\n  mutate(\r\n    book = case_when(\r\n      row_number() %in% 1:20  ~ \"Water\",\r\n      row_number() %in% 21:40 ~ \"Earth\",\r\n      TRUE                    ~ \"Fire\"\r\n    )\r\n  ) %>% \r\n  group_by(book) %>% \r\n  mutate(\r\n    chapter_num = row_number()\r\n  ) %>% \r\n  ungroup() %>%\r\n  mutate(\r\n    book_num = case_when(\r\n      book == \"Water\" ~ 1,\r\n      book == \"Earth\" ~ 2,\r\n      book == \"Fire\"  ~ 3\r\n    )\r\n  ) %>% \r\n  select(chapter, chapter_num, book, book_num)) %>% \r\n  mutate(id = row_number(),\r\n         character = ifelse(character == \"\", \"Scene Description\", character))\r\n\r\n\r\n\r\nFinally, we’ll split up the text column into character_words and scene_description. We’ll have to use some more stringr functions and some regular expressions, but it won’t be too bad.\r\n\r\n\r\ndat_transcript <- dat %>% \r\n  mutate(scene_description = str_extract_all(text, pattern = \"\\\\[[^\\\\]]+\\\\]\"),\r\n         character_words = str_remove_all(text, pattern = \"\\\\[[^\\\\]]+\\\\]\"),\r\n         character_words = ifelse(character == \"Scene Description\",\r\n                                  NA_character_, \r\n                                  str_trim(character_words))) %>% \r\n  select(id, book, book_num, chapter, chapter_num,\r\n         character, full_text = text, character_words, scene_description)\r\n\r\ndat_transcript\r\n\r\n\r\n# A tibble: 13,389 x 9\r\n      id book  book_num chapter chapter_num character full_text\r\n   <int> <chr>    <dbl> <chr>         <int> <chr>     <chr>    \r\n 1     1 Water        1 The Bo~           1 Katara    \"Water. ~\r\n 2     2 Water        1 The Bo~           1 Scene De~ \"As the ~\r\n 3     3 Water        1 The Bo~           1 Sokka     \"It's no~\r\n 4     4 Water        1 The Bo~           1 Scene De~ \"The sho~\r\n 5     5 Water        1 The Bo~           1 Katara    \"[Happil~\r\n 6     6 Water        1 The Bo~           1 Sokka     \"[Close-~\r\n 7     7 Water        1 The Bo~           1 Scene De~ \"Behind ~\r\n 8     8 Water        1 The Bo~           1 Katara    \"[Strugg~\r\n 9     9 Water        1 The Bo~           1 Scene De~ \"The bub~\r\n10    10 Water        1 The Bo~           1 Katara    \"[Exclai~\r\n# ... with 13,379 more rows, and 2 more variables:\r\n#   character_words <chr>, scene_description <list>\r\n\r\nNow we have all of the transcript data in a nice and tidy dataframe! Hooray! We could stop there, but we might as well get more data.\r\nWriters and Directors\r\nLet’s scrape the writers and directors data from pages like this:\r\n\r\nI won’t go over the code as much because it is pretty similar to what we did before. Just remember the philosophy: do it once, then extract the pattern and do that same thing for everything.\r\n\r\n\r\niceberg_overview <- read_html(\"https://avatar.fandom.com/wiki/The_Boy_in_the_Iceberg\")\r\n\r\niceberg_overview %>% \r\n  html_nodes(\".pi-border-color\") %>% \r\n  html_text() %>% head(5)\r\n\r\n\r\n[1] \"Information\\n\\n\\n\\t\\n\\t\\tSeries\\n\\t\\n\\tAvatar: The Last Airbender\\n\\n\\n\\n\\t\\n\\t\\tBook\\n\\t\\n\\tWater\\n\\n\\n\\n\\t\\n\\t\\tEpisode\\n\\t\\n\\t1/61\\n\\n\\n\\n\\t\\n\\t\\tOriginal air date\\n\\t\\n\\tFebruary 21, 2005\\n\\n\\n\\n\\t\\n\\t\\tWritten by\\n\\t\\n\\t<U+200E>Michael Dante DiMartino, Bryan KonietzkoAdditional writing: Aaron Ehasz, Peter Goldfinger, Josh Stolberg\\n\\n\\n\\n\\t\\n\\t\\tDirected by\\n\\t\\n\\tDave Filoni\\n\\n\\n\\n\\t\\n\\t\\tAnimation\\n\\t\\n\\tJM Animation\\n\\n\\n\\n\\t\\n\\t\\tGuest stars\\n\\t\\n\\tMako (Uncle), Melendy Britt (Gran Gran)\\n\\n\\n\\n\\t\\n\\t\\tProduction number\\n\\t\\n\\t101\\n\\n\\n\"                                     \r\n[2] \"\\n\\t\\n\\t\\tSeries\\n\\t\\n\\tAvatar: The Last Airbender\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \r\n[3] \"\\n\\t\\n\\t\\tBook\\n\\t\\n\\tWater\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        \r\n[4] \"\\n\\t\\n\\t\\tEpisode\\n\\t\\n\\t1/61\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \r\n[5] \"\\n\\t\\n\\t\\tOriginal air date\\n\\t\\n\\tFebruary 21, 2005\\n\"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \r\n\r\n# in addition to being an episode,\r\n# Jet is a character and Lake Laogai is a place\r\n# so the links have _(episode) at the end\r\n\r\nchapter_urls2 <- chapters %>% \r\n  mutate(chapter = str_replace_all(chapter, pattern = \" \", replacement = \"_\"),\r\n         chapter = str_replace_all(chapter, pattern = \"\\'\", replacement = \"%27\"),\r\n         chapter = case_when(\r\n           chapter == \"Jet\"         ~ \"Jet_(episode)\",\r\n           chapter == \"Lake_Laogai\" ~ \"Lake_Laogai_(episode)\",\r\n           TRUE                     ~ chapter\r\n           \r\n         )) %>% \r\n  pull(chapter)\r\n\r\n# remember full_urls? overview_urls is just like that\r\n\r\noverview_urls <- glue(\"https://avatar.fandom.com/wiki/{chapter_urls2}\")\r\n\r\n\r\n\r\nDo it once.\r\n\r\n\r\noverview_urls[32] %>% \r\n  read_html() %>% \r\n  html_nodes(\".pi-border-color\") %>% \r\n  html_text() %>%\r\n  .[6:7] %>% \r\n  enframe() %>% \r\n  mutate(value = str_trim(value)) %>% \r\n  separate(col = value, into = c(\"role\", \"name\"), sep = \" by\") %>% \r\n  mutate(name = str_trim(name)) %>% \r\n  pivot_wider(names_from = role, values_from = name)\r\n\r\n\r\n# A tibble: 1 x 2\r\n  Written                                  Directed       \r\n  <chr>                                    <chr>          \r\n1 Joshua Hamilton, Michael Dante DiMartino Ethan Spaulding\r\n\r\nThen iterate.\r\n\r\n\r\ntic()\r\nwriters_directors <- overview_urls %>% \r\n  map_dfr( ~ read_html(.x) %>% html_nodes(\".pi-border-color\") %>% \r\n    html_text() %>% .[6:7] %>% enframe() %>% \r\n    mutate(value = str_trim(value)) %>% \r\n    separate(col = value, into = c(\"role\", \"name\"), sep = \" by\") %>% \r\n    mutate(name = str_trim(name)) %>% \r\n    pivot_wider(names_from = role, values_from = name) %>% \r\n    mutate(url = .x)\r\n  )\r\ntoc()\r\n\r\n\r\n10.08 sec elapsed\r\n\r\n# some data wrangling/cleaning\r\n\r\nwriters_directors2 <- writers_directors %>%\r\n  mutate(\r\n    url = str_remove(url, \"https://avatar.fandom.com/wiki/\"),\r\n    url = str_replace_all(url, \"_\", \" \"),\r\n    url = str_remove(url, \" \\\\(episode\\\\)\"),\r\n    url = str_replace_all(url, pattern = \"%27\", replacement = \"\\'\"),\r\n    Written = str_replace(Written, \" Additional writing: \", \", \")\r\n  ) %>% \r\n  rename(chapter = url, writer = Written, director = Directed)\r\n\r\ndat2 <- dat_transcript %>% \r\n  left_join(\r\n    writers_directors2, by = \"chapter\"\r\n  )\r\n\r\nglimpse(dat2)\r\n\r\n\r\nRows: 13,389\r\nColumns: 11\r\n$ id                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ...\r\n$ book              <chr> \"Water\", \"Water\", \"Water\", \"Water\", \"Wa...\r\n$ book_num          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n$ chapter           <chr> \"The Boy in the Iceberg\", \"The Boy in t...\r\n$ chapter_num       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n$ character         <chr> \"Katara\", \"Scene Description\", \"Sokka\",...\r\n$ full_text         <chr> \"Water. Earth. Fire. Air. My grandmothe...\r\n$ character_words   <chr> \"Water. Earth. Fire. Air. My grandmothe...\r\n$ scene_description <list> [<>, <>, \"[Close-up of Sokka as he gri...\r\n$ writer            <chr> \"<U+200E>Michael Dante DiMartino, Bryan Koniet...\r\n$ director          <chr> \"Dave Filoni\", \"Dave Filoni\", \"Dave Fil...\r\n\r\nGot the writers and directors. Now we just want the imdb ratings.\r\n\r\nImdb Ratings\r\n\r\nThe ratings were from IMDB instead of the Avatar Wiki, so the names are slightly different. Here’s what we need to do for the last bit of data collection:\r\nFirst, we’ll scrape the chapter names again and alter a few of them. Second, we’ll scrape the ratings. Lastly, we’ll bind the chapters and ratings data together, right join on the original chapters data, and then finally join the data to the main transcript data.\r\n\r\n\r\nimdb_raw <- read_html(\"https://www.imdb.com/list/ls079841896/\")\r\n\r\nchapter_names <- chapters %>% pull(chapter) %>% \r\n  str_flatten(collapse = \"|\")\r\n\r\nimdb_chapters <- imdb_raw %>% \r\n  html_nodes(\"h3.lister-item-header\") %>% \r\n  html_text() %>% \r\n  enframe() %>% \r\n  mutate(value = str_extract(value, pattern = chapter_names),\r\n         value = case_when(\r\n           name == 29 ~ \"Winter Solstice, Part 2: Avatar Roku\",\r\n           name == 44 ~ \"The Boiling Rock, Part 1\",\r\n           name == 52 ~ \"Winter Solstice, Part 1: The Spirit World\",\r\n           TRUE       ~ value\r\n         ))\r\n\r\nimdb_ratings <- imdb_raw %>% \r\nhtml_nodes(\"div.ipl-rating-widget\") %>% \r\n  html_text() %>% \r\n  enframe() %>% \r\n  mutate(value = parse_number(value))\r\n\r\nimdb <- bind_cols(imdb_chapters, imdb_ratings) %>% \r\n  select(chapter = 2, rating = 4) %>% \r\n  right_join(chapters) %>% \r\n  select(chapter, imdb_rating = rating)\r\n\r\ndat3 <- dat2 %>% left_join(imdb, by = \"chapter\")\r\n\r\n\r\n\r\nHooray! We are finally done.\r\n\r\n\r\nglimpse(dat3)\r\n\r\n\r\nRows: 13,389\r\nColumns: 12\r\n$ id                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ...\r\n$ book              <chr> \"Water\", \"Water\", \"Water\", \"Water\", \"Wa...\r\n$ book_num          <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n$ chapter           <chr> \"The Boy in the Iceberg\", \"The Boy in t...\r\n$ chapter_num       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n$ character         <chr> \"Katara\", \"Scene Description\", \"Sokka\",...\r\n$ full_text         <chr> \"Water. Earth. Fire. Air. My grandmothe...\r\n$ character_words   <chr> \"Water. Earth. Fire. Air. My grandmothe...\r\n$ scene_description <list> [<>, <>, \"[Close-up of Sokka as he gri...\r\n$ writer            <chr> \"<U+200E>Michael Dante DiMartino, Bryan Koniet...\r\n$ director          <chr> \"Dave Filoni\", \"Dave Filoni\", \"Dave Fil...\r\n$ imdb_rating       <dbl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA,...\r\n\r\nConclusion\r\nDoing all of this work would be almost pointless if it wasn’t shared. Fortunately, the data can now be installed via GitHub:\r\n\r\n\r\n# install.packages(devtools)\r\n# install_github(\"averyrobbins1/appa)\r\n\r\n\r\n\r\nNow you can do cool stuff like make a Sokka word cloud.\r\n\r\n\r\nlibrary(ggwordcloud)\r\nlibrary(tidytext)\r\n\r\ndata(\"stop_words\")\r\n\r\ndat <- appa::appa\r\n\r\nset.seed(123)\r\n\r\n# Sokka's top 100 words\r\n\r\ndat %>% \r\n  filter(character == \"Sokka\") %>% \r\n  unnest_tokens(word, character_words) %>% \r\n  anti_join(stop_words) %>% \r\n  count(word, sort = TRUE) %>% \r\n  slice(1:100) %>%\r\n  ggplot(aes(label = word, size = n, color = n)) +\r\n    geom_text_wordcloud() +\r\n    scale_size_area(max_size = 20) +\r\n    scale_color_viridis_c()\r\n\r\n\r\n\r\n\r\nThat’s all for now! Thank you for reading, and I hope you learned something that you found helpful!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-13-webscraping-atla/webscraping-atla_files/figure-html5/unnamed-chunk-21-1.png",
    "last_modified": "2021-02-20T23:12:17-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-07-11-avatar-eda/",
    "title": "Exploring Avatar: The Last Airbender transcript data",
    "description": "Here is my initial EDA of the Avatar: The Last Airbender data that I scraped from the Avatar Wiki, now available in the \"appa\" R package.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-07-11",
    "categories": [],
    "contents": "\r\n\r\nQuarantine plus Avatar on Netflix? I surrender!\r\nMy wife and I recently finished a beloved show that we each really enjoyed watching growing up - Avatar: The Last Airbender. Since its arrival on Netflix, I have spoken with a number of people who have once again fallen in love with this show from their childhood. To be honest, watching Avatar as an adult was probably even more enjoyable!\r\nInstead of hiding my nerd-self from the world, I will embrace it here for all to see by combining my fondness for Avatar with my love of all things data and R. I hope you find this entertaining!\r\nAlso, if you don’t care about R code, but are just a fan of the show, scroll past the code and get to the insights!\r\nSadly, Aang never mastered databending\r\nAs always, we’ll install and/or load our needed packages, and read in our data.\r\n\r\n\r\n# packages\r\n\r\n# install.packages(c(\"tidyverse\", \"tidytext\", \"textdata, \"DataExplorer\", \"devtools\",\r\n# \"ggrepel\", \"wordcloud\"))\r\n# devtools::install_github('cttobin/ggthemr')\r\n# devtools::install_github(\"averyrobbins1/sometools\")\r\n# devtools::install_github(\"averyrobbins1/appa\")\r\n\r\nlibrary(tidyverse) # all the things\r\nlibrary(tidytext) # text analysis using tidy principles\r\nlibrary(ggrepel) # easy text labels for ggplot2 plots\r\n#library(ggthemr) # nice ggplot2 themes\r\nlibrary(sometools) # my personal R package\r\nlibrary(glue) # run code inside of strings\r\nlibrary(gghighlight) # easily highlight data in a ggplot2\r\nlibrary(patchwork) # composing multiple ggplot2 plots\r\nlibrary(wordcloud) # make wordclouds\r\n\r\n# set plot theme\r\n\r\n#ggthemr('fresh')\r\n\r\n# data\r\n\r\ndat <- appa::appa\r\n\r\nglimpse(dat)\r\n\r\n\r\nRows: 13,385\r\nColumns: 12\r\n$ id                <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, ...\r\n$ book              <fct> Water, Water, Water, Water, Water, Wate...\r\n$ book_num          <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n$ chapter           <fct> The Boy in the Iceberg, The Boy in the ...\r\n$ chapter_num       <int> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\r\n$ character         <chr> \"Katara\", \"Scene Description\", \"Sokka\",...\r\n$ full_text         <chr> \"Water. Earth. Fire. Air. My grandmothe...\r\n$ character_words   <chr> \"Water. Earth. Fire. Air. My grandmothe...\r\n$ scene_description <list> [<>, <>, \"[Close-up of the boy as he g...\r\n$ writer            <chr> \"<U+200E>Michael Dante DiMartino, Bryan Koniet...\r\n$ director          <chr> \"Dave Filoni\", \"Dave Filoni\", \"Dave Fil...\r\n$ imdb_rating       <dbl> 8.1, 8.1, 8.1, 8.1, 8.1, 8.1, 8.1, 8.1,...\r\n\r\nWait… an R package named appa? Named after our favorite sky bison? That’s right. Having a lot of free time on a weekend means webscraping a bunch of Avatar data, tidying it up, and putting it in an R package. If you want access to the dataset used here, just install it from GitHub:\r\n\r\n\r\ndevtools::install_github(\"averyrobbins1/appa\")\r\n\r\n\r\n\r\nI will upload a post documenting the webscraping journey soon! But first, let’s answer some Avatar questions.\r\nImdb ratings\r\nThe best and the worst\r\nThe dataset from appa include imdb ratings for each chapter (epsiode). What were people’s favorite chapters? Let’s plot chapters with ratings, and label some of the best and worst.\r\n\r\n\r\ndat_ratings <- dat %>%\r\n    distinct(book, chapter_num, chapter, imdb_rating) %>% \r\n    group_by(book) %>% \r\n    mutate(\r\n        best_worst = case_when(\r\n            imdb_rating %in%\r\n                (slice_max(., order_by = imdb_rating, n = 1) %>% \r\n                pull(imdb_rating)) ~ \"best\",\r\n            imdb_rating %in%\r\n                (slice_min(., order_by = imdb_rating, n = 1) %>% \r\n                pull(imdb_rating)) ~ \"worst\",\r\n            TRUE                  ~ \"mid\"\r\n        )\r\n    ) %>% \r\n    ungroup()\r\n\r\ndat_ratings %>% \r\n    ggplot(aes(x = chapter_num, y = imdb_rating)) +\r\n    geom_point(aes(color = book), size = 2) +\r\n    geom_text_repel(\r\n        dat_ratings %>% filter(best_worst != \"mid\"),\r\n        mapping = aes(label = chapter),\r\n        seed = 1, size = 3.25, alpha = .75, force = 5,\r\n        direction = \"both\") +\r\n    facet_wrap(~ book) +\r\n    labs(\r\n        x = \"Chapters\",\r\n        y = \"Imdb Ratings\",\r\n        color = \"Book\",\r\n        title = \"What were the highest and lowest rated chapters in each book?\") +\r\n    scale_y_continuous(breaks = seq(from = 7, to = 10, by = .5))\r\n\r\n\r\n\r\n\r\nIt’s kind of funny to see The Great Divide, Avatar Day, and Nightmares and Daydreams towards the lower end of the ratings. I remember each of those feeling sort of like “filler” episodes; they didnt’ contribute much to the main characters’ growth or the overall plot of the show. Also, its no surprise that most of the epic season finales were highly rated!\r\n\r\n\r\n\r\nThe Fire book was on fire\r\nMy wife and I also came to the consensus that book 3 was the best, followed by book 2. It seems that most people would agree.\r\n\r\n\r\ndat_ratings %>% \r\n    ggplot(aes(x = chapter_num, y = imdb_rating, color = book)) +\r\n    geom_jitter(width = .5, height = 0) +\r\n    geom_smooth(se = FALSE) +\r\n    labs(\r\n        x = \"Chapters\",\r\n        y = \"Imdb Ratings\",\r\n        color = \"Book\",\r\n        title = \"It only gets better over time!\")\r\n\r\n\r\n\r\n\r\nTranscripts\r\nSo far we have just been looking at imdb ratings, but most of the data that we have available are actually the transcripts. Let’s do some basic text analysis.\r\nWhat are the most commonly spoken words by all the characters?\r\n\r\n\r\ndata(\"stop_words\")\r\n\r\ndat_tidy <- dat %>% \r\n  select(book, chapter, chapter_num, character,\r\n         character_words, imdb_rating) %>% \r\n  filter(character != \"Scene Description\") %>% \r\n  group_by(book) %>% \r\n  mutate(line_num = row_number()) %>% \r\n  ungroup() %>% \r\n  unnest_tokens(word, character_words)\r\n\r\ndat_tidy2 <- dat_tidy %>% \r\n  anti_join(stop_words)\r\n\r\ndat_tidy2 %>% \r\n  count(word, sort = TRUE) %>%\r\n  slice(1:20) %>% \r\n  mutate(word = fct_reorder(word, n)) %>% \r\n  ggplot(aes(x = n, y = word)) +\r\n  geom_col() +\r\n  geom_text(aes(label = n), nudge_x = 12) +\r\n  labs(title = \"Most common words spoken in Avatar: The Last Airbender\",\r\n       x = \"Count\",\r\n       y = \"Words Spoken\") +\r\n  gghighlight(word %in% c(\"fire\", \"appa\", \"uncle\"))\r\n\r\n\r\n\r\n\r\nAfter filtering out all of the stop words, here are my first impressions:\r\nIt is significant that “fire” is the most spoken word in the series. Fire is symbolic of many things, including life and death. In the series especially, it is inseparably connected to the ultimate obstacle to world peace, the Fire Nation, and to Aang’s greatest foe, the Fire Lord.\r\nMy first thought as to the reason of the high occurence of the word appa: “Appa, yip, yip!”\r\nWow! I love that “uncle” made the top 20 most common words. Uncle Iroh is definitely one of my favorite characters. He is very wise and beloved by all.\r\nI am curious to know if pretty much all of those mentions of “uncle” Iroh are from his nephew zuko. Let’s find out.\r\n\r\n\r\ndat_tidy2 %>% \r\n  filter(word == \"uncle\") %>% \r\n  count(character, sort = TRUE) %>% \r\n  slice(1:10) %>% \r\n  mutate(character = fct_reorder(character, n)) %>% \r\n  ggplot(aes(x = n, y = character)) +\r\n  geom_col() +\r\n  geom_text(aes(label = n), nudge_x = 1.5) +\r\n  gghighlight(character == \"Zuko\") +\r\n  labs(title = \"Who said uncle?\", x = \"Word Count\", y = \"Characters\")\r\n\r\n\r\n\r\n\r\nWell, that certainly checks out haha. Zuko was often with his uncle. This also makes me want to see all of the most common words of each of our favorite characters.\r\n\r\n\r\nfav_characters <- c(\"Aang\", \"Katara\", \"Sokka\", \"Toph\", \"Iroh\", \"Zuko\")\r\n\r\n# let's make a quick helper function\r\n\r\ntop_words <- function(fav){\r\n  character_plot <- dat_tidy2 %>% \r\n  filter(character == fav) %>% \r\n  count(word, sort = TRUE) %>%\r\n  slice(1:10) %>% \r\n  mutate(word = fct_reorder(word, n)) %>% \r\n  ggplot(aes(x = n, y = word)) +\r\n  geom_col() +\r\n  geom_text(aes(label = n), size = 2.75, nudge_x = -4) +\r\n  labs(title = glue(\"{fav}\"), y = NULL, x = NULL)\r\n}\r\n\r\nplots <- map(fav_characters, top_words)\r\n\r\npatched_plots <- patchwork::wrap_plots(plots)\r\n\r\nwords <- c(\"katara\",\"aang\",\"fire\", \"toes\", \"tea\", \"honor\")\r\n\r\nindex <- 1:6\r\n\r\nwrap_plots(\r\n  map2(\r\n    index, words, ~ patched_plots[[.x]] + gghighlight(word == .y)\r\n    )\r\n  ) + plot_annotation(title = \"Most common words spoken by main characters\")\r\n\r\n\r\n\r\n\r\nWell, Aang and Katara are certainly concerned with each other. Sokka was often focused on the war effort and the Fire Nation. Toph could see with her feet, and of course Iroh really loved his tea! Sadly, Zuko was just obsessed with his getting his honor from his father for the longest time.\r\nLet’s look more into the characters and how much they speak in general and in each episode. Note we are going back to the dataset including stop words.\r\n\r\n\r\ndat_character_words <- dat_tidy %>% \r\n  filter(character != \"Scene Description\") %>% \r\n  count(chapter, chapter_num, character, imdb_rating) %>% \r\n  add_count(character, wt = n, name = \"word_count\") %>% \r\n  filter(word_count > 5000)\r\n\r\ndat_character_words %>% \r\n  count(character) %>% \r\n  ggplot(aes(x = reorder(character, desc(n)) , y = n)) +\r\n  geom_col() +\r\n  labs(\r\n    title = \"Sokka sure is mouthy - probably all those jokes!\",\r\n    x = NULL, y = \"Word Count\"\r\n  ) + gghighlight(character == \"Sokka\")\r\n\r\n\r\n\r\n\r\nLet’s look into any relationships that may exist between certain characters speaking and episode ratings.\r\n\r\n\r\np1 <- dat_character_words %>% \r\n  ggplot(aes(x = chapter_num, y = imdb_rating, color = character, size = n)) +\r\n  geom_point() +\r\n  facet_wrap(~ character) +\r\n  labs(\r\n    x = \"Chapters\", y = \"Imdb Rating\",\r\n    size = \"Word Count\"\r\n  ) + guides(color = FALSE)\r\n\r\np2 <- dat_character_words %>% \r\n  ggplot(aes(x = n, y = imdb_rating, color = character)) +\r\n  geom_point() +\r\n  geom_smooth(se = FALSE) +\r\n  facet_wrap(~ character) +\r\n  labs(\r\n    x = \"Word Count\", y = \"Imdb Rating\"\r\n  ) + guides(color = FALSE)\r\n\r\np1 / p2 + \r\n  plot_annotation(\r\n    title = \"Do specific character's word count seem to affect chapter ratings?\")\r\n\r\n\r\n\r\n\r\nNo crazy patterns jump out. There could be some slight linear relationships here, but we would want to run some models to get more certain results. This will suffice for now.\r\nQuick sentiment analysis\r\nLet’s use a sentiment lexicon (basically a dictionary to evaluate the emotion of a text) to get a feel for the tone of the show.\r\nWe’ll break each book up into segments of 50 lines each and analyze each segment. We’ll create a new column, sentiment, which is the difference between positive and negative word counts per segment. Also, let’s look at the entire transcript with and without the stop words.\r\n\r\n\r\ndat_bing <- get_sentiments(\"bing\")\r\n\r\ndat_bing %>% head(3)\r\n\r\n\r\n# A tibble: 3 x 2\r\n  word     sentiment\r\n  <chr>    <chr>    \r\n1 2-faces  negative \r\n2 abnormal negative \r\n3 abolish  negative \r\n\r\ndat_sent <- dat_tidy %>% inner_join(dat_bing)\r\n\r\ndat_sent_books <- dat_sent %>% \r\n  count(book, index = line_num %/% 50, sentiment) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n) %>% \r\n  mutate(sentiment = positive - negative)\r\n\r\np3 <- dat_sent_books %>% \r\n  ggplot(aes(x = index, sentiment, fill = book)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ book) +\r\n  labs(\r\n    title = \"With stop words\", y = \"Sentiment\", x = \"Index\"\r\n  )\r\n\r\ndat_sent2 <- dat_tidy2 %>% inner_join(dat_bing)\r\n\r\ndat_sent_books2 <- dat_sent2 %>% \r\n  count(book, index = line_num %/% 50, sentiment) %>%\r\n  pivot_wider(names_from = sentiment, values_from = n) %>% \r\n  mutate(sentiment = positive - negative)\r\n\r\np4 <- dat_sent_books2 %>% \r\n  ggplot(aes(x = index, sentiment, fill = book)) +\r\n  geom_col(show.legend = FALSE) +\r\n  facet_wrap(~ book) +\r\n  labs(\r\n    title = \"Without stop words\", y = \"Sentiment\", x = \"Index\"\r\n  )\r\n\r\np3 / p4\r\n\r\n\r\n\r\n\r\nIt looks like stop words are mostly positive, which doesn’t tell us much. I think the transcripts without the stop words would give greater insight into the overall sentiment of each book. When we look at segments without the stop words, the overall sentiment is negative. I would guess that that has to do with the whole 100 year war and everything. That being said, most people would argue that the message of Avatar: The Last Airbender is one of hope, friendship, and good overcoming evil.\r\nAnd a wordcloud just for fun\r\n\r\n\r\ndat_tidy2 %>%\r\n  count(word, sort = TRUE) %>%\r\n  with(wordcloud(word, n, max.words = 100))\r\n\r\n\r\n\r\n\r\nConclusion\r\nThere is so much more potential here! This was honestly just the tip of the iceberg. I want to do more EDA, and I think it could be interesting to dive deeper into specific characters. Stayed tuned for more analysis and Avatar related fun in the future. Thanks for reading!\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-07-11-avatar-eda/avatar-eda_files/figure-html5/unnamed-chunk-3-1.png",
    "last_modified": "2021-02-20T23:00:17-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 768
  },
  {
    "path": "posts/2020-06-17-automation-and-organization/",
    "title": "Become more efficient through automation and better organization",
    "description": "Write functions, iterate with purrr, and stay organized to save time and work more efficiently!",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-06-17",
    "categories": [],
    "contents": "\r\nMotivation\r\nRecently, while reading through a book about creating R Packages, I was especially impacted by a few lines concerning automation: anything that can be automated, should be automated. Do as little as possible by hand. Do as much as possible with functions. This philosophy has vast implications, and has caused me to reflect on different ways that I can reduce inefficiency in my work.\r\nIn addition to automating where possible, keeping your projects well organized will also make you more efficient. I am certainly not the only one who has sifted through poorly named files and folders trying find some data or script.\r\nThe goal of this post is to save you both time and headaches. I will highlight a few practices and methods that I have found useful, and I would encourage you to seek out and share similar tips with the community! I’ll assume you have some basic experience with the tidyverse.\r\n\r\n\r\n# install.packages(c(\"tidyverse\",\"fs\"))\r\nlibrary(tidyverse) # all the things\r\nlibrary(fs) # for working with files\r\n\r\n\r\n\r\nLet’s dive right in.\r\nAutocomplete and intentional naming of variables\r\nTake advantage of RStudio’s fabulous autocomplete! Be intentional with naming objects and functions. While working as a tutor in a lab for R programming and data science, I would often see the most interesting and inconsistent variable names:\r\n\r\n\r\n# bad\r\n\r\nWednesdayHousingData_Project\r\nData_That_I_Cleaned\r\nABBYLANEmanor2\r\n\r\n\r\n\r\nGenerally, variable names should strike a nice balance between being concise and descriptive. If you want, you could lean more towards descriptive names since it is easier to type more with RStudio’s autocomplete.\r\nPick a naming convention and stick with it. I personally use snake case (all lower case letters separated by underscores). snake_case_is_arguably_easier toReadThanCamelCase. You can choose whatever you want, just be consistent.\r\nPersonally, I always name datasets dat (for data), or give them dat as a prefix. Some people prefer df, dt or something else. Consistency with a name or prefix makes it a lot easier to type and find objects.\r\n\r\n\r\n# good\r\n\r\ndat\r\ndat_tidy\r\ndat_train\r\ndat_nested\r\n\r\n# even longer names are okay because of autocomplete\r\n\r\ndat_linkedin_profiles\r\ndat_canvas_long\r\ndat_canvas_wide\r\n\r\n\r\n\r\nTo be fair, dat itself isn’t super descriptive, but it is great when working with just one dataset or when used as a prefix for modified versions of a dataset. With RStudio’s autocomplete, even if you forget what something is called, you can always type dat… and all your datasets will appear.\r\n\r\n\r\n# simple example of a workflow\r\n\r\ndat_courses <- read_csv(\"path-to-data/data1.csv\")\r\n\r\ndat_sections <- read_csv(\"path-to-data/data2.csv\")\r\n\r\ndat <- dat_courses %>% \r\n    left_join(dat_sections)\r\n\r\ndat %>% glimpse()\r\n\r\n# comments about what I did to go from dat to dat2\r\n# blah blah blah\r\n\r\ndat2 <- dat %>%\r\n    select(column1, column2, column3) %>% \r\n    filter(column3 > 50) %>% \r\n    mutate(\r\n        # lots of code\r\n    )\r\n\r\ndat2 %>% glimpse()\r\n\r\n# comments about what I did to go from dat2 to dat_final\r\n# blah blah blah\r\n\r\ndat_final <- dat2 %>% \r\n    group_by(column1) %>% \r\n    summarise(\r\n        # all sorts of summaries\r\n    )\r\n\r\ndat_final %>% glimpse()\r\n    \r\nwrite_rds(dat_final, \"path/data.rds\")\r\n\r\n\r\n\r\n\r\nFunctions and iteration\r\nAs an R user, writing functions and iterating with the purrr package just seemed too formidable for a long time. After a lot of study and practice, however, I am far more confident in those areas. If you want to learn how to leverage these tools too, now is the time!\r\nI will outline the basics here, and point you towards additional resources at the end of this post. I mainly want to highlight some examples that you may find helpful.\r\nFunctions\r\n\r\n\r\n# basic outline for writing a function\r\n\r\nname <- function(variables) {\r\n    # code here\r\n}\r\n\r\n\r\n\r\n\r\n\r\ndat <- mtcars %>% as_tibble()\r\ndat %>% glimpse()\r\n\r\n\r\nRows: 32\r\nColumns: 11\r\n$ mpg  <dbl> 21.0, 21.0, 22.8, 21.4, 18.7, 18.1, 14.3, 24.4, 22.8...\r\n$ cyl  <dbl> 6, 6, 4, 6, 8, 6, 8, 4, 4, 6, 6, 8, 8, 8, 8, 8, 8, 4...\r\n$ disp <dbl> 160.0, 160.0, 108.0, 258.0, 360.0, 225.0, 360.0, 146...\r\n$ hp   <dbl> 110, 110, 93, 110, 175, 105, 245, 62, 95, 123, 123, ...\r\n$ drat <dbl> 3.90, 3.90, 3.85, 3.08, 3.15, 2.76, 3.21, 3.69, 3.92...\r\n$ wt   <dbl> 2.620, 2.875, 2.320, 3.215, 3.440, 3.460, 3.570, 3.1...\r\n$ qsec <dbl> 16.46, 17.02, 18.61, 19.44, 17.02, 20.22, 15.84, 20....\r\n$ vs   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1...\r\n$ am   <dbl> 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1...\r\n$ gear <dbl> 4, 4, 4, 3, 3, 3, 3, 4, 4, 4, 4, 3, 3, 3, 3, 3, 3, 4...\r\n$ carb <dbl> 4, 4, 1, 1, 2, 1, 4, 2, 2, 4, 4, 3, 3, 3, 4, 4, 4, 1...\r\n\r\nSay there is some code that I run frequently during an analysis:\r\n\r\n\r\ndat %>% \r\n  group_by(cyl) %>% \r\n  summarize(\r\n    n = n(), sd = sd(mpg), median = median(mpg), mean = mean(mpg)\r\n  )\r\n\r\n\r\n# A tibble: 3 x 5\r\n    cyl     n    sd median  mean\r\n* <dbl> <int> <dbl>  <dbl> <dbl>\r\n1     4    11  4.51   26    26.7\r\n2     6     7  1.45   19.7  19.7\r\n3     8    14  2.56   15.2  15.1\r\n\r\nI use it so much, that I want to wrap it up into a function that I’ll name get_summary. I should look at my code and ask myself: what are the inputs? In this case, I have the data (dat), a variable that I want to group the summary by (cyl), and I also have the variable that I actually want a summary of (mpg). I would list those inputs as arguments inside of function(), and give them easy to understand names. Then I’d swap out the hard coded variable names with the argument names I just came up with. Thus, cyl becomes grouping_var and mpg becomes summary_var, both in function() and the main code body.\r\nSince I am writing a function with tidyverse verbs, I also need to wrap the variables in the main body of code with {{ }}. More on that later. Lastly, remember that the main body of the code is nestled between curly braces {}. With all of that said, we end up with this:\r\n\r\n\r\nget_summary <- function(data, grouping_var, summary_var) {\r\n  data %>% \r\n    group_by( {{ grouping_var }} ) %>% \r\n    summarize(\r\n      n = n(),\r\n      sd = sd( {{ summary_var }} ),\r\n      median = median( {{ summary_var }} ),\r\n      mean = mean( {{ summary_var }} )\r\n    )\r\n}\r\n\r\ndat %>% get_summary(cyl, mpg)\r\n\r\n\r\n# A tibble: 3 x 5\r\n    cyl     n    sd median  mean\r\n* <dbl> <int> <dbl>  <dbl> <dbl>\r\n1     4    11  4.51   26    26.7\r\n2     6     7  1.45   19.7  19.7\r\n3     8    14  2.56   15.2  15.1\r\n\r\ndat %>% get_summary(am, wt)\r\n\r\n\r\n# A tibble: 2 x 5\r\n     am     n    sd median  mean\r\n* <dbl> <int> <dbl>  <dbl> <dbl>\r\n1     0    19 0.777   3.52  3.77\r\n2     1    13 0.617   2.32  2.41\r\n\r\nNow I can use the get_summary function with all sorts of variables! A detailed explanation as to why we use the curly-curly {{ }} operator here is beyond the scope of this humble post, but I would point you towards the two best resources that I have found that explain why we need them: Programming with dplyr and rlang 0.4.0. In short, using them allows us to write things like cyl and mpg when calling the function, instead of dat$cyl or dat$mpg. Having to use {{ }} may seem slightly painful at first, but it allows us to create functions that are much more user friendly, just like functions from the tidyverse.\r\nIteration\r\nNow, onto iterating with purrr. Introducing map.\r\nmap is basically a for loop in a function. It allows you to apply a function to many things at once.\r\n\r\n\r\n# the map function\r\n\r\nmap(my_list, my_function)\r\n\r\n# my_list is a list, vector, or data.frame/tibble that you want to iterate over\r\n# my_function is a function that will be applied to each element of my_list\r\n\r\n\r\n\r\nInstead of this:\r\n\r\n\r\nmean(dat$mpg)\r\nmean(dat$disp)\r\nmean(dat$hp)\r\n# and so on...\r\n\r\n\r\n\r\ndo this:\r\n\r\n\r\ndat %>% map(mean) # or map(dat, mean)\r\n\r\n\r\n$mpg\r\n[1] 20.09062\r\n\r\n$cyl\r\n[1] 6.1875\r\n\r\n$disp\r\n[1] 230.7219\r\n\r\n$hp\r\n[1] 146.6875\r\n\r\n$drat\r\n[1] 3.596563\r\n\r\n$wt\r\n[1] 3.21725\r\n\r\n$qsec\r\n[1] 17.84875\r\n\r\n$vs\r\n[1] 0.4375\r\n\r\n$am\r\n[1] 0.40625\r\n\r\n$gear\r\n[1] 3.6875\r\n\r\n$carb\r\n[1] 2.8125\r\n\r\nHmm, I don’t love that the output is a list of numbers. Let’s use map_dbl to return a vector of doubles instead. map always returns a list, but there are other map_* variants that give you more control over the output.\r\nRun ?purrr::map in the console for more info.\r\n\r\n\r\ndat %>% map_dbl(mean)\r\n\r\n\r\n       mpg        cyl       disp         hp       drat         wt \r\n 20.090625   6.187500 230.721875 146.687500   3.596563   3.217250 \r\n      qsec         vs         am       gear       carb \r\n 17.848750   0.437500   0.406250   3.687500   2.812500 \r\n\r\nmap and its friends can iterate over data.frames and tibbles because the columns are just vectors of equal length. Also, notice we just type mean and not mean(). Alternatively we could write:\r\n\r\n\r\ndat %>% map(~ mean(.x))\r\n\r\n\r\n\r\nUse the ~ to signal that you are writing a function and the pronoun .x to represent each element of the list. For those familiar with for loops, the .x is similar to the i in for i in ....\r\nYou can also use map2 to iterate over two vectors or lists at the same time. ?map2\r\n\r\n\r\na <- 1:5\r\nb <- 5:1\r\na\r\n\r\n\r\n[1] 1 2 3 4 5\r\n\r\nb\r\n\r\n\r\n[1] 5 4 3 2 1\r\n\r\nmap2(a, b, ~ .x * .y)\r\n\r\n\r\n[[1]]\r\n[1] 5\r\n\r\n[[2]]\r\n[1] 8\r\n\r\n[[3]]\r\n[1] 9\r\n\r\n[[4]]\r\n[1] 8\r\n\r\n[[5]]\r\n[1] 5\r\n\r\nmap2_dbl(a, b, ~ .x * .y)\r\n\r\n\r\n[1] 5 8 9 8 5\r\n\r\nIn this case, .x represents the first list, and .y represent the second. See ?pmap for help with 3 or more arguments.\r\nSome personal examples\r\nYou can use helper functions that you have created in combination with functions from purrr to reduce duplication in your code and automate processes that are more manual.\r\nThe other day I found myself doing this a lot:\r\n\r\n\r\ndat %>% count(cyl)\r\n\r\n\r\n# A tibble: 3 x 2\r\n    cyl     n\r\n* <dbl> <int>\r\n1     4    11\r\n2     6     7\r\n3     8    14\r\n\r\ndat %>% count(vs)\r\n\r\n\r\n# A tibble: 2 x 2\r\n     vs     n\r\n* <dbl> <int>\r\n1     0    18\r\n2     1    14\r\n\r\ndat %>% count(am)\r\n\r\n\r\n# A tibble: 2 x 2\r\n     am     n\r\n* <dbl> <int>\r\n1     0    19\r\n2     1    13\r\n\r\nUnfortunately, using map with count is not as straightforward as map(dat, count), so I wrote a function to make things easier.\r\n\r\n\r\ncount_values <- function(data, ...) {\r\n    select(data, ...) %>% \r\n        \r\n        # the ... let you select columns like normal in dplyr::select\r\n        \r\n      mutate(across(everything(), as.character)) %>%  \r\n        \r\n        # dplyr::across is from dplyr 1.0.0 \r\n        # dplyr::mutate_all(as.character) would do the same thing \r\n        \r\n      map_dfr(~ count(tibble(value = .x), value, sort = TRUE), .id = \"variable\")\r\n    \r\n        # purrr::map_dfr outputs a row-binded tibble\r\n}\r\n\r\ndat %>% count_values(cyl, vs, am)\r\n\r\n\r\n# A tibble: 7 x 3\r\n  variable value     n\r\n  <chr>    <chr> <int>\r\n1 cyl      8        14\r\n2 cyl      4        11\r\n3 cyl      6         7\r\n4 vs       0        18\r\n5 vs       1        14\r\n6 am       0        19\r\n7 am       1        13\r\n\r\n\r\nAutomate the tedious\r\nOne way to stay organized is by keeping different scripts and data in appropriately named folders. When I start fresh in a new R project, I usually create the same folders every time. I wrote a function to make this easier:\r\n\r\n\r\nget_started <- function() {\r\n  dirs <- list(\"data-raw\", \"data-derived\", \"eda\", \"scripts\", \"models\", \"helpers\")\r\n  purrr::walk(dirs, fs::dir_create)\r\n}\r\n# walk is like map, but only calls the function for its side-effect\r\n# dir_create creates a new directory\r\n\r\n\r\n\r\nget_started outputs:\r\n\r\nIf you wanted more control over which folders to create, you could use get_started2 and just input what you want.\r\n\r\n\r\nget_started2 <- function(...) {\r\n  dirs <- list(...)\r\n  purrr::walk(dirs, fs::dir_create)\r\n}\r\n\r\nget_started2(\"data\", \"reports\", \"personal-folders\", \"analysis\")\r\n\r\n\r\n\r\n\r\nHold on to your helpers\r\nOver the course of an analysis, or just your R user life in general, you will probably write a bunch of helper function to ahem help you. If you don’t create your own R package to keep those functions, the next best thing is to keep them each in their own script with some comments documenting what they do/how to use them.\r\nIf I had all of my helper functions in a directory called helpers, I could source all of them with the code below:\r\n\r\n\r\nfs::dir_ls(\"helpers\") %>% walk(source)\r\n\r\n# dir_ls lists all of the files in a directory\r\n\r\n\r\n\r\nFiles and folders\r\nFrom the names of the directories in get_started and get_started2, can you guess what sort of files should go in each? I hope so. Names of files and folders should be easy to understand for both humans and machines. Stick with numbers, lower case letters, _, and -. That’s it. Avoid funky characters and spaces. Can you image how much better the world would be if everyone did just that?\r\nI generally like to use _ for separating groups of things that are bigger ideas or are not necessarily related, and - for everything else. Here is an example of a typical project directory organized with files and folders:\r\ndata-raw\r\n2020-03-26_canvas-assignments.csv\r\n2020-04-05_canvas-assignments.csv\r\n2020-04-18_canvas-assignments.csv\r\nstudent-total.csv\r\n\r\ndata-derived\r\ncanvas-tidy-current.rds\r\nstudent-seniors.rds\r\n\r\nscripts\r\ncanvas-tidy-current.R\r\nstudent-seniors.R\r\n\r\nhelpers\r\ncount-values.R\r\nget-started.R\r\n\r\nmodels\r\ncanvas-decision-tree.R\r\ncanvas-random-forest.R\r\nstudent-seniors-log-regr.R\r\nstudent-seniors-xgboost.R\r\n\r\neda\r\n01_canvas-api.R\r\n02_canvas-courses.R\r\n03_canvas-assignments.R\r\n\r\nResources for learning more\r\nHere are some of my favorite books and tutorials that helped me to learn more about writing my own functions and iterating with purrr:\r\nFunctional Programming\r\nRStudio Primers, namely the Iterate and Write Functions sections\r\nChapters 19 and 21 of R for Data Science\r\nFor more help with a project-oriented workflow:\r\nProject-oriented workflow\r\nFor more help with managing file paths:\r\nhere package\r\nThank you for reading!\r\nI truly hope that something in this post as helped you! Investing time and effort into learning how to write functions, iterate with purrr, and staying well organized will pay off greatly. Stay safe and happy coding!\r\n\r\n\r\n\r\n",
    "preview": {},
    "last_modified": "2021-02-20T22:51:59-07:00",
    "input_file": {}
  },
  {
    "path": "posts/2020-05-02-packages-for-eda/",
    "title": "Useful R Packages for Exploratory Data Analysis",
    "description": "Here are some helpful packages and functions to make EDA in R a bit easier.",
    "author": [
      {
        "name": "Avery Robbins",
        "url": "www.linkedin.com/in/avery-robbins"
      }
    ],
    "date": "2020-05-02",
    "categories": [],
    "contents": "\r\nEffective exploratory data analysis (EDA) is crucial to most any data science project. For a great first look at how to do EDA in R, check out the 7th chapter of R for Data Science. This post here will point you towards some useful tools to make some aspects of EDA easier and faster.\r\nPrepared to be primed\r\nOver the past few months I have found myself using a few packages or functions over and over again whenever I get my hands on a new dataset. I also have recently stumbled upon some new beauties that I think are worth sharing. This post is meant to be more of a primer than a real deep dive into any one package. Links to learn more about each package/function are included throughout.\r\n\r\n\r\n# if needed:\r\n# install.packages(c(\"tidyverse\", \"janitor\", \"DataExplorer\", \"skimr\", \"trelliscopejs\", \"gapminder\"))\r\n\r\nlibrary(tidyverse) # (dplyr, ggplot2, %>%)\r\nlibrary(janitor)\r\nlibrary(DataExplorer)\r\nlibrary(skimr)\r\nlibrary(trelliscopejs)\r\nlibrary(gapminder)\r\n\r\ndat <- ggplot2::diamonds\r\n# learn more about diamonds dataset: ?diamonds\r\n\r\n\r\n\r\nAn oldie but a goodie\r\ndplyr::glimpse()\r\nWhen I first look at a new dataset, I really just want to take a peak, or a glimpse of the data. glimpse from dplyr is perfect for just that. It shows you all the basics of your dataset: number of rows and columns, names and types of variables, and the first several values in each row.\r\n\r\n\r\nglimpse(dat)\r\n\r\n\r\nRows: 53,940\r\nColumns: 10\r\n$ carat   <dbl> 0.23, 0.21, 0.23, 0.29, 0.31, 0.24, 0.24, 0.26, 0...\r\n$ cut     <ord> Ideal, Premium, Good, Premium, Good, Very Good, V...\r\n$ color   <ord> E, E, E, I, J, J, I, H, E, H, J, J, F, J, E, E, I...\r\n$ clarity <ord> SI2, SI1, VS1, VS2, SI2, VVS2, VVS1, SI1, VS2, VS...\r\n$ depth   <dbl> 61.5, 59.8, 56.9, 62.4, 63.3, 62.8, 62.3, 61.9, 6...\r\n$ table   <dbl> 55, 61, 65, 58, 58, 57, 57, 55, 61, 61, 55, 56, 6...\r\n$ price   <int> 326, 326, 327, 334, 335, 336, 336, 337, 337, 338,...\r\n$ x       <dbl> 3.95, 3.89, 4.05, 4.20, 4.34, 3.94, 3.95, 4.07, 3...\r\n$ y       <dbl> 3.98, 3.84, 4.07, 4.23, 4.35, 3.96, 3.98, 4.11, 3...\r\n$ z       <dbl> 2.43, 2.31, 2.31, 2.63, 2.75, 2.48, 2.47, 2.53, 2...\r\n\r\nI prefer glimpse over head because glimpse simply gives you more information. Also, when working in an R script, using glimpse is often nicer than using View because View takes you out of the script and can slightly distrupt your flow. I have also had View crash R when being used on very large datasets.\r\nglimpse also works great with the pipe %>%. I like to use it at the end of a series of manipulations on data as a sort of sanity check.\r\n\r\n\r\n# random dplyr code\r\ndat %>% \r\n    rename(length = x, width = y) %>% \r\n    mutate(price_euro = price * .91) %>% \r\n    filter(carat > .7) %>% \r\n    select(carat, cut, price_euro) %>% glimpse()\r\n\r\n\r\nRows: 26,778\r\nColumns: 3\r\n$ carat      <dbl> 0.86, 0.71, 0.78, 0.96, 0.73, 0.80, 0.75, 0.75...\r\n$ cut        <ord> Fair, Very Good, Very Good, Fair, Very Good, P...\r\n$ price_euro <dbl> 2508.87, 2510.69, 2510.69, 2510.69, 2511.60, 2...\r\n\r\n\r\nsnake_case_is_the_best\r\njanitor::clean_names()\r\nThe clean_names() function from the janitor package is awesome for cleaning up annoying column names. You just pipe in your data and it magically converts your columns to snake case. There are other options, too. ?clean_names. Okay maybe this function doesn’t really have much to do with EDA, but quickly standardizing all of your column names sure makes working with them easier.\r\n\r\n\r\ndat_ugly_names <- tribble(\r\n    ~\"BAD Column\", ~\"Good name?\", ~\"This-hurts_Me\",\r\n    \"a\",            1,              \"fruit\",\r\n    \"b\",            2,              \"taco\",\r\n    \"c\",            3,              \"corona virus\"\r\n)\r\n\r\ndat_ugly_names %>% clean_names()\r\n\r\n\r\n# A tibble: 3 x 3\r\n  bad_column good_name this_hurts_me\r\n  <chr>          <dbl> <chr>        \r\n1 a                  1 fruit        \r\n2 b                  2 taco         \r\n3 c                  3 corona virus \r\n\r\n# I use clean_names just about every time I import data from somewhere!\r\n\r\n# import dataset from the wild:\r\n# dat <- read_csv(\"some-crazy-data-from-the-wild.csv) %>% clean_names()\r\n# oh wow, now this wild dataset at least has some tame column names\r\n\r\n\r\n\r\nData the Explora-data\r\nI am not proud of that subheading. Enter the DataExplorer package to redeem myself. This is a package I plan on diving deeper into myself. Lots of golden nuggets here. From this package, I have used plot_histogram and plot_bar for quite a while. Just this last week (at the time of writing this) I learned about profile_missing and create_report. Finally, while writing this post, I learned about another useful function called introduce.\r\nintroduce and profile_missing just take data as an argument, while the other functions allow you to customize the outputs a bit more if desired.\r\nplot_histogram() - Creates histograms for all continuous variables in a dataset.\r\n\r\n\r\ndat %>% plot_histogram()\r\n\r\n\r\n\r\n\r\nplot_bar() - Creates bar charts for all discrete variables in a dataset.\r\n\r\n\r\ndat %>% plot_bar()\r\n\r\n\r\n\r\n\r\nprofile_missing() - Tells you the number and percentage of NA values from each of the columns in a dataset.\r\n\r\n\r\ndatasets::airquality %>% profile_missing() # datasets from base R\r\n\r\n\r\n  feature num_missing pct_missing\r\n1   Ozone          37  0.24183007\r\n2 Solar.R           7  0.04575163\r\n3    Wind           0  0.00000000\r\n4    Temp           0  0.00000000\r\n5   Month           0  0.00000000\r\n6     Day           0  0.00000000\r\n\r\ncreate_report() - Compiles a whole bunch of data profiling statistics (including outputs from the three above functions, correlation between variables, etc.) into an html report. It looks like you can customize it a bunch, but the default report has been sufficient for me (except for setting a y so that a response variable can be included in some of the plotting functions). I won’t include an example here because it produces an html document, but you could probably run create_report(mtcars) or something in your console to see what it outputs. Lots of good stuff here.\r\nintroduce() - Describes basic info about the data.\r\n\r\n\r\ndat %>% introduce()\r\n\r\n\r\n# A tibble: 1 x 9\r\n   rows columns discrete_columns continuous_colu~ all_missing_col~\r\n  <int>   <int>            <int>            <int>            <int>\r\n1 53940      10                3                7                0\r\n# ... with 4 more variables: total_missing_values <int>,\r\n#   complete_rows <int>, total_observations <int>, memory_usage <dbl>\r\n\r\nHmm. Seeing how the output is formatted, I don’t like it as much. Too wide. I think I would rather use it in combination with glimpse.\r\n\r\n\r\ndat %>% introduce() %>% glimpse()\r\n\r\n\r\nRows: 1\r\nColumns: 9\r\n$ rows                 <int> 53940\r\n$ columns              <int> 10\r\n$ discrete_columns     <int> 3\r\n$ continuous_columns   <int> 7\r\n$ all_missing_columns  <int> 0\r\n$ total_missing_values <int> 0\r\n$ complete_rows        <int> 53940\r\n$ total_observations   <int> 539400\r\n$ memory_usage         <dbl> 3457760\r\n\r\n\r\nSkim a bit right off the top\r\nskimr::skim()\r\nFrom the skimr package, “skim() is an alternative to summary(), quickly providing a broad overview of a data frame.” Now that I use functions from DataExplorer a lot, I don’t use skim as much, but some people might like it.\r\n\r\n\r\nskim(iris)\r\n\r\n\r\nTable 1: Data summary\r\nName\r\niris\r\nNumber of rows\r\n150\r\nNumber of columns\r\n5\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n1\r\nnumeric\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\nVariable type: factor\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nordered\r\nn_unique\r\ntop_counts\r\nSpecies\r\n0\r\n1\r\nFALSE\r\n3\r\nset: 50, ver: 50, vir: 50\r\nVariable type: numeric\r\nskim_variable\r\nn_missing\r\ncomplete_rate\r\nmean\r\nsd\r\np0\r\np25\r\np50\r\np75\r\np100\r\nhist\r\nSepal.Length\r\n0\r\n1\r\n5.84\r\n0.83\r\n4.3\r\n5.1\r\n5.80\r\n6.4\r\n7.9\r\n▆▇▇▅▂\r\nSepal.Width\r\n0\r\n1\r\n3.06\r\n0.44\r\n2.0\r\n2.8\r\n3.00\r\n3.3\r\n4.4\r\n▁▆▇▂▁\r\nPetal.Length\r\n0\r\n1\r\n3.76\r\n1.77\r\n1.0\r\n1.6\r\n4.35\r\n5.1\r\n6.9\r\n▇▁▆▇▂\r\nPetal.Width\r\n0\r\n1\r\n1.20\r\n0.76\r\n0.1\r\n0.3\r\n1.30\r\n1.8\r\n2.5\r\n▇▁▇▅▃\r\n\r\nYou could also pipe this into summary.\r\n\r\n\r\nskim(iris) %>% summary()\r\n\r\n\r\nTable 2: Data summary\r\nName\r\niris\r\nNumber of rows\r\n150\r\nNumber of columns\r\n5\r\n_______________________\r\n\r\nColumn type frequency:\r\n\r\nfactor\r\n1\r\nnumeric\r\n4\r\n________________________\r\n\r\nGroup variables\r\nNone\r\n\r\nHonorable mention\r\ntrelliscopejs::facet_trelliscope\r\nWow, just wow. Go here to check out how to use facet_trelliscope. This function combines the awesomeness of faceting in ggplot2 with the additional interactive power of javascript. I will definitely be exploring trelliscopejs some more. I love what people come up with.\r\nConclusion\r\nI hope some of this has been useful to you. Most of the functions I mentioned can probably help get you started with EDA. They are especially useful when you take an initial look at a dataset, and perhaps you could continue to use some of these functions during the EDA process. However, simple functions like these do not replace best practices that you have been taught. Hopefully they just support you in whatever your process looks like.\r\nAgain, I would point you towards R for Data Science to learn more about EDA in R, and also the 4th chapter of Feature Engineering and Selection for EDA that is more focused on building models.\r\nThank you so much for reading my first post! Feel free to share this with anyone who might find it helpful or leave a comment pointing towards other useful packages.\r\n\r\n\r\n\r\n",
    "preview": "posts/2020-05-02-packages-for-eda/packages-for-eda_files/figure-html5/unnamed-chunk-5-1.png",
    "last_modified": "2021-02-20T21:58:52-07:00",
    "input_file": {},
    "preview_width": 1248,
    "preview_height": 672
  }
]
